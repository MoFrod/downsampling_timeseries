---
title: "Reading Notes"
output: pdf_document
citation_package: natbib
bibliography: "references.bib"
biblio-style: "apa"
link-citations: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
```

```{r ProjectTemplate, include = FALSE}
library(ProjectTemplate)
load.project()
```

##TVStore @TVStore

**Abstract**

A pressing demand emerges for storing extreme-scale time series data, which are widely generated by industry and research at an increasing speed. Automatically constraining data storage can lower expenses and improve performance, as well as saving storage maintenance efforts at the resourceconstrained conditions. However, two challenges exist: 1) how to preserve data as much and as long as possible within the storage bound; and, 2) how to respect the importance of data that generally changes with data age.

To address the above challenges, we propose time-varying compression that respects data values by compressing data to functions with time as input. Based on time-varying compression, we prove the fundamental design choices regarding when compression must be initiated to guarantee bounded storage. We implement a storage-bounded time series store TVStore based on an open-source time series database. Extensive evaluation results validate the storageboundedness of TVStore and its time-varying pattern of compression on both synthetic and real-world data, as well as demonstrating its efficiency in writes and queries.

**Introduction**

- "Time series databases are becoming the most popular type of databases in recent years." Newest data seems to suggest otherwise? @DB-engines

- "... the fast increasing volume of time series data has placed an unprecedented requirement on computing resources, especially storage space [6, 79]." pg 83

- "as the significance of time series data is highly correlated with the age of the data [22,37,89], it is desirable to have a storage management strategy that takes data ages into account [3, 7]." pg 83

- " Significant prior work has addressed the storage-control problem by compression, which can be lossless or lossy. Lossless compression [10, 33, 57, 71, 73] preserves the complete data, but its achievable upper bound on compression ratio [93] might not be satisfactory for applications." pg 83

- "Hence, time series databases commonly control storage consumption by directly discarding data older than a given time [43] or exceeding a storage threshold [67]. But discarding historical data causes a loss [94]." pg 83

- "Another common approach is to exploit lossy compression [15,41,65], which preserves partial data and trades off precision for space. But existent approaches to lossless and lossy compression are only best-effort about the final size of compressed data size [13, 24, 99]." pg 83

- *Problem statement* "We consider the problem of automatically bounding the storage of a time series store by compression. To enable this, our key insight is that time series data can be compressed losslessly or lossily according to its importance, which is in turn related to its age, as users commonly accept information loss on less important old data [12, 14, 23, 38, 40]." pg 83

- "The goal is to reduce computing resource consumption and improve performance." pg 84

**Background and Motivation**

- "...mounting demands have emerged for keeping time series data for future analysis [94]. But time series data are generated at a growing speed that is outpacing the increase of computing capabilities [17, 79]. Many application scenarios cannot afford enough computing resources such as storage and network bandwidth to accommodate the processing needs for time series data." pg 84

- "Sensors of a connected car can generate about 30 terabytes
(TB) of data per day [62, 77].... Since a 30TB disk can cost around $1200, a month’s worth of data can fill up a 960TB disk, causing a cost of $30,000." pg 84

- "In the oil and gas industry, a typical offshore oil platform generates more than 1TB of data [19] daily. But common data transmission via satellite connection allows only a speed from 64 Kbps to 2Mbps for these offshore oil platforms. If all data are transmitted
back for processing, it would take more than 12 days to move 1 day’s worth of data to the processing backend [8]." pg 84

- "... cosmological simulations generate petabytes of data per simulation run [34] and climate simulations generate tens of terabytes per second [29]." pg 84

- "Data reduction is necessary to enable data processing and analytics within a reasonable amount of resource and time [92]." pg 84

- "The importance of time series data changes along with time, as reflected by applications’ favoring recent data over old data [5, 18, 31], or favoring some events at certain moments over others [49,83]." pg 85

- "As a result, we have seen a plethora of research on data series analysis and prediction considering the timechanging pattern [9,22,36,37,89]." pg 85

- "As time series data can be identified by timestamps, we use a time-dependent function to denote the changing importance of data. Hence, the compression ratios can also be deduced from the function." pg 85

- "We must deduce the proper moments for compression initiation when 1) it is not too late that the storage space is exceeded during compression; and, 2) it is not too early that unnecessary compression is applied to some recent data or that an improperly high compression ratio is used." pg 85

- "Challenges: As a result, two main challenges exist in automatically bounding the time series storage by timevarying compression: 1) how time-varying compression can be executed on an ever-increasing volume of data and with error bounds computed, when the compression ratios
keep changing... and, 2) how to automatically decide the conditions for running time-varying compression such that storage space is always bounded but not too much..." pg 85

- time-varying compression (TVC)

- "To guarantee that data are compressed to the compression ratio sequence r~1~,r~2~,...,r~k~, TVC groups r~i~ data chunks into the *i*th segment. Each segment is then compressed to an output chunk. Hence, the *i*th chunk of the compression output has a compression ratio r~i~, complying to the definition of *r(t)*." pg 86

- "...given the data chunks to be compressed in round *n*, TVC *virtually* decompresses them, by mathematical mapping, to the original raw data chunks for computing the sequence of compression ratios. Then, the conditions for ratio sequencing are considered." pg 86

- "Virtual decompression enables the generation of a compression ratio sequence based on the original raw data even after rounds of compression. Only by virtual decompression could the compressed data always follow the time-dependent function *r(t)*’s definition. Otherwise, data can only be compressed following the exponentially decaying pattern, as compression on compressed data leads to the multiplication of compression ratios. This would limit the applications of TVC, as *r(t)* can only be an exponential function." pg 86

-  "Keeping data at an extremely high error rate is no better than discarding it. Therefore, TVC allows users to specify a compression
ratio *r~max~* or an error rate *e~max~*. TVC automatically discards
data compressed at a ratio higher than *r~max~* or at an error rate
larger than *e~max~*." pg 87

- "Lossless and lossy compression methods exist for time series data. Lossless compression can reconstruct the original data accurately. For lossless compression, specialized compressors for integer [10, 33,
55, 73] and for floating-point values [57, 71] outperform general-purpose compressors [20,28,32,58,66]. It is common practice that general-purpose compression is applied along with specialized compressors [43, 54, 96] in time series stores." pg 94

- "To manage the ever-increasing volume of time series data, most time series stores either have a native architecture to support a distributed deployment such as InfluxDB [43], or exploit a distributed storage for scalability, e.g., KairosDB [47] on Cassandra [53], TimescaleDB [90] on PostgreSQL database [72], Druid [100] on HDFS [35],
BtrDB [4] on Ceph [97], Chronix [54] on Solr [84], and Peregreen on an object store [94]."

- "Besides lossless and lossy compression, time series databases commonly exploit the data retention policy to reclaim storage space by removing data exceeding a time period [42] or exceeding a storage quota, e.g., RRDtool [67], for further storage reduction."

## MSc Thesis @Sveinn

**Abstract**

- "The focus of this thesis is to explore methods for downsampling data
which can be visualized in the form of a line chart, for example, time series." pg vii

**Introduction**

- "If values on the x-axis are evenly spaced over time, the data is most often referred to as a time series." pg 1

- "A line chart entailing a data point distribution high in density proffers limited information to an observer of that chart." pg 1

- "...the focus of this thesis is mainly to explore downsampling algorithms which return a subset of data points existing in the original data." pg 2

- "sometimes it might suffice to use only every other data point
or maybe every tenth data point, depending on the data and canvas size. Still, such a method is only suitable if the data is “smooth” in nature and has little or no fluctuations. If the data is somewhat irregular, then using only every nth data point to draw the line chart will almost guarantee that many interesting peaks and troughs will not be included in the chart..." pg 2

- "...it is important to emphasize that the downsampled data is only intended to visually represent the original data for human observation
and not data analysis, statistical or otherwise...When processing
information for visual representation, it is only important to retain the data which offers the most information and is actually perceived by people, the rest can be discarded." pg 2

- "The primary objective is to design and implement a few algorithms that can effectively downsample any data which can be directly drawn as line chart, with special emphasis on time series. The downsampling algorithms then need to be compared in regard to several factors. Those factors include but are not limited to efficiency, complexity and correctness." pg 3

- "The second objective is to be able to say something about the human perception on downsampled data drawn as a line chart. That includes conducting a simple comparative survey using different downsampling algorithms.! pg 3

**Intutive Downsampling Algorithms**

- *Mode-Median-Bucket:* "The algorithm uses primarily two methods, mode and median, to govern which data points are returned, thus the name Mode-Median-Bucket. The bucket part in the algorithm name refers to the data being split up into buckets, each containing approximately equal number of data points. The algorithm then finds one data point within each bucket as follows. If there is a single y-value which has the highest frequency (the mode) then the leftmost corresponding data point is selected. If no such data point exists a point corrosponding to the median of the y-values is selected. An exception to these rules is when a global peak or a global trough is found within the bucket. This is to ensure that those points are included in the downsampled
data." pg 5

- "One of the most obvious issues with this algorithm is that it is very likely to exclude local peaks and troughs within each bucket (see figure 2.1). The reason for this is because it does not take into account what the y-value actual is, only how frequent it is within each bucket. The only exception is when the global peak or trough occur in the bucket."pg 6

- "Another minor issue arises if the global peak and trough both occur in the same bucket. Then the peak is always used no matter what the value of the trough is. Incidentally, the absolute height of the trough might be much greater than that of the peak." pg 7

- *Min-Std-Error-Bucket:* "It is based on linear
regression and uses the formula for the standard error of the estimate (SEE) to downsample data. The SEE is a measure of the accuracy of predictions made with a regression line when a linear least squares technique is applied. The greater the error the more discrepancy between the line and the data points. Of course one line through all data is not very useful. Thus, the original data needs to be split up into buckets and then multiple lines calculated between points in the buckets." pg 8

- "it is worth mentioning that it is not really a practical solution, merely an exploration of using certain concepts, e.g., the standard error. Even if the algorithm produces a statistically good line chart, it is not a very good visual representation since it smooths out many of the peaks and troughs of the line chart." pg 8 

- "In downsampling, it is only normal to miss some minor fluctuation but when major fluctuations are skipped, the line chart can suffer perceptually, giving a poor representation of the original chart." pg 11

- "Another downside to this algorithm is that it has a lot of loose ends. There are other ways to solve the dynamic optimization problem and maybe a more greedy approach is adequate. Finding the absolute minimal sum of the standard errors might not be necessary when a fairly low sum might suffice." pg 12

- *Longest-Line-Bucket:* "It starts off exactly the same [as the MSEB algorithm], splitting the data points into buckets and calculating lines going through all the points in one bucket and all the points in the next bucket... The main difference is that instead of calculating the standard error for each line segment it simply calculates its length (Euclidean distance between the two points defining the line). Then, as with the MSEB algorithm, the points and lines segments are converted to a directed acyclic graph (DAG) and the weight of an edge is the length of the corresponding line segment... All that remains is to find the longest path through the graph. The path will contain one point per bucket which forms the longest total line length through all the bucket." pg 13

- "Finding the longest path in a general graph is a NP-Hard problem and cannot be computed in polynomial time. However in the case of the graph being a DAG, the edge weight can simply be changed to its negation, thus changing it to a shortest path problem. This problem can then be solved with dynamic programming in exacly the same way as done in the MSEB algorithm, except that maximization rather than minimization is applied." pg 13

- "...like with the MSEB algorithm the problem with this algorithm is how complicated and inefficient the current implementation is. If this algorithm is to become a practical option in a real application, it would need to be simplified and optimized." pg 14

**Cartographic Generalization**

- "There are several well known techniques in cartographic generalization regarding polyline simplification. The evaluation of these algorithms is still an issue of active research and there is large number of different spatial measurements which can be
used as criteria [9]." pg 15

- "One of the most common line simplification method is the Douglas-Peucker algorithm [5]. It selects the point that is over a specified threshold and furthest away from an imaginary line which is initially between the first and the last point of the polyline. The algorithm then calls itself recursively with the polylines on both sides of the selected point (including the selected point) until all the points have been either selected or discarded (fall under the threshold). When the recursion is done the points which were selected define the simplified polyline." pg 15

- "Another, more recent, algorithm to simplify lines is called the Visvalingam–Whyatt algorithm [11]. The basic idea behind this algorithm is to give all the points a certain rank based on their significance. The significance of a point is the area size of a triangle which it forms with its two adjacent points on each side. This is usually called the effective area of a point... The least significance points are then excluded in the simplified line and the effective areas of the remaining adjacent points recalculated as it has changed." pg 16

- "One obvious restriction is that the algorithm should not skip too many data points in a row. That is because that would result in long line segments over parts in the line chart with minor fluctuations. Such minor fluctuations are indeed information and their presence has a noticeable perceptual impact for a line chart." pg 16

- "The perception of line charts and other data visualization techniques has been the topic of discussion and research for a long time [3, 6]. The scope of the research is very broad and often borders on the field of psychology [2]. For now it suffices to say that when downsampling data to be displayed as line chart, it is important to retain as much visual characteristics of the line chart as possible and not suggest any false patterns because minor fluctuations are indeed a type of visual characteristic." pg 17 

**Largest Triangle Algorithms**

- *Largest-Triangle-One-Bucket:* "This algorithm is very simple. First all the points are ranked by calculating their effective areas. Points with effective areas as null are excluded. The data points are then split up into approximately equal number of buckets as the specified downsample threshold. Finally, one point with the highest rank (largest effective area) is
selected to represent each bucket in the downsampled data." pg 19

- "One issue with this algorithm is that the ranking (effective area) of a point only depends on its two adjacent points. It is perhaps not an apparent issue but it can lead to bad representation of the original line chart in certain cases." pg 20

- *Largest-Triangle-Three-Buckets:* "With the algorithm
discussed in this section, the effective area of a point does not depend on the position of its two adjacent points but on the points in the previous and next buckets, making the possible effective area much larger. The first step is to divide all the data points into buckets of approximately equal size. The first and last buckets however contain only the first and last data points of the original data... This is to ensure that those points will be included in the downsampled data. The next step is to go through all the buckets from the first to the last and select one point from each bucket. The first bucket only contains a single point so it is selected by default. The next bucket would then normally contain more than one point from which to choose." pg 21

- "The algorithm works with three buckets at a time and proceeds from left to right. The first point which forms the left corner of the triangle (the effective area) is always fixed as the point that was previously selected and one of the points in the middle bucket shall be selected now...add a temporary point to the last bucket and keep it fixed. That way the algorithm has two fixed points; and one only needs to calculate the number of triangles equal to the number of points in the current bucket. The point in the current bucket which forms the largest triangle with these two fixed point in the adjacent buckets is then selected." pg 22

- "There is still the matter of how this temporary point in the next bucket should be decided. A simple idea is to use the average of all the points in the bucket." pg 23

- "The biggest problem is not really how the points are selected within the buckets but rather how the points are divided into buckets. This algorithm uses roughly equal sized buckets (except for the first and last buckets) to make sure a point is selected for every fixed interval on the x-axis. The problem is that some line charts have
somewhat irregular fluctuations." pg 24

- "The problem is that not all buckets can be visually represented fairly with just one point and some buckets might not even need to be represented at all (if the local fluctuation is very small). This is at least the case if all the buckets have approximately the same number of points and the algorithm selects only one point. from each bucket. This problem also exists in all the previous algorithms which
rely on this roughly equal bucket dividing concept." pg 24

- *Largest-Triange-Dynamic:* "...this algorithm does not rely on equal size buckets but allows a dynamic bucket size. If a part of the line chart is fluctuating greatly, the buckets become smaller; and if another part is relatively calm, the buckets become larger. The algorithm is really an attempt to address the biggest problem with the LargestTriangle-Three-Buckets (LTTB) algorithm..." pg 25

- "The first step is to assign a number to all buckets which indicates whether a bucket needs to be smaller or larger, if only one point is supposed to represent the bucket. An obvious way to calculate this number is to apply a simple linear regression for all buckets." pg 26

- "If the SSE for a bucket is relatively high, it means that the data points within the bucket would most likely be better represented as two buckets. If however the SSE is relatively low for a bucket, it can be merged with either one of the adjacent buckets, if one of them also has a relatively low SSE. After all the initial buckets have been ranked, the next step is to either split or merge them accordingly." pg 26

 - "After a given number of iterations the algorithm needs to halt (on some predetermined condition)." pg 27
 
 - "For example, if 1,000 points are to be downsampled down to 900 points, the algorithm runs 11 iterations. If however the 1,000 points need to be downsampled down to 50, the algorithm runs 200 iterations." pg 28
 
 
 - "Although this algorithm can produce a good visual representation of a line chart, it is not without issues. As mentioned before, this algorithm gives the best results if the data is irregular. When the data is highly regular over the whole line chart, the algorithm appears no better than the LTTB algorithm (it might even sometimes be
a little worse)." pg 29
 
- "Perhaps the main problem has to do with how this algorithm could be optimized for better performance because it is currently rather slow in comparison with the LTTB algorithm." pg 30
 
 - "Another issue is determining the halting condition. As it is implemented, the halting condition is calculated with a simple formula which takes in the data point count and downsample threshold. It does not take into account the nature of the line chart." pg 30
 
 **Survey**
 
 - "In order to collect more data the survey was designed in such a
way that people were asked to order the downsampled lines charts from the best representation to the worst. Thus, the survey would yield some information about all choices of the downsampled line charts and also how good or bad a representation a downsampled line chart is relative to the other choices." pg 31
 
**Questions**

- "In order to keep the survey short it only consisted of nine questions (not including two questions about participant’s age and education). In each question the participant was shown the original line chart and a number of downsampled line charts (displayed in a random order for each participant) using different methods and settings. The participant’s task was then to order the downsampled line charts from the best to the worst representation relative to the original line chart." pg 32
 
- "One of the assumptions of this thesis is that both aesthetics and resemblance matter and this survey was designed to give some insight into what a good visual representation really means for people in the context of downsampled line charts." pg 33
 
**Participants** 

- "The survey was open to everybody and 58 participants took part over the course of about two weeks. DataMarket advertised the survey on their Twitter page and the University of Iceland also sent out emails to all students in computer science and software engineering." pg 35

**Survey Results**

-  The Largest-Triangle-Dynamic algorithm yielded the best results according to the survey. This comes as no surprise since extreme downsampling of irregular data is one of its strong points. Not far behind was the Longest-Line-Bucket algorithm. That was a bit surprising seeing as it is one of the intuitively designed algorithms, with little theoretical foundations (at least in the papers I reviewed)." pg 43

**Overview Comparison of the Downsampling Algorithms**

- Speed and scalability pg 45

- Complexity and portability pg 45

- Correctness pg 46

**Conclusion**

- "As it turned out, the Largest-Triangle-Three-Buckets algorithm (see section 4.2) was both efficient and produced good results for most
cases. Already it is being put to use by DataMarket and is reported to be a great improvement over their previous method of downsampling data (see section 2.1)." pg 49

- "For future work, it might be prudent to explore other polyline simplification algorithms applied in cartographic generalization to adapt them for the purpose of simplifying line charts. Furthermore, similar to cartographic generalization, what is needed is a deeper understanding of how line charts are perceived [11], what characteristics have the most perceptual impact and to what degree." pg 50


**References**