---
class: 'article'
usepackage: '{fancyhdr}'
pagestye: '{fancy}'
fancyhead: '[C]{09/06/2023}'
fancyfoot: '[C]{\thepage}'
title: 'CSC8639 Interim Report: Explaining Time Series Downsampling'
author: 'Author: 210431461 | Supervisor: Matthew Forshaw'
citation_package: natbib
bibliography: 'bib.bib'
biblio-style: 'apalike'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
```

```{r ProjectTemplate, include = FALSE}
library(ProjectTemplate)
load.project()
```

The interim report is a short document which provides a brief overview of your proposed project and the progress you have made on it so far.

## Introduction (10%)

Decision-makers must trust that the data being considered sufficiently represents the situation they are deciding on. Trusting the data means trusting What data is selected, how the chosen data is collected and stored, and the capability of data practitioners to understand the quality, insights and limitations of this data. It is already difficult to determine and explain that the data reliably and truthfully reflects the situation in question across every phase of this data processing and analysis pipeline. The increasing volume of data today makes this more difficult [reference].

This problem is particularly pertinent to collections of observations obtained through repeated measurements over time @AusBureau, known as time series data. This data type is being "widely generated by industry and research at an increasing speed" @TVStore. This is putting unprecedented demand on resources (look up 6, 79 in @TVStore) and forcing data practitioners to select methods, such as aggregation, windowing, and downsampling, that reduce this volume to align with cost or time limitations, storage capabilities, and sustainability ambitions @Sveinn, @TVStore, @Shift. These reduction methods involve discarding data, which could result in the loss of important information and reduce the representativeness of the data.

However, reducing time series data is a vital part of visualising this data for human observation @Sveinn. Downsampling reduces "...the number of data points while preserving the overall shape of the time series" @MinMaxLTTB, allowing the human eye to observe only the most valuable data points. It is popular to visualise these data points a simple line graph @MinMaxOrdered. These graphs effectively convey the overall shape of the time series data (no 2 for @MinMaxLTTB), but they offer little insight into which downsampled time series is most representative of the original data.

The research outlined by this interim report is exploring how to better explain the impact of downsampling time series data on its representativeness. It is hoped that this will help data practitioners confidently select their downsampling approach, better communicate the insights and limitations of downsampled data, and, in doing so, support decision-makers to trust the data they are considering. 

## Aim and Objectives (20%)

The aim of this research is to help improve how the downsampling of time series data is explained for data practitioners and decision-makers. To achieve this, the research objectives include:

- Compare current downsampling algorithms to develop a baseline understanding of their impact on the original data. 

- Undertake exploratory data analysis to identify common properties of time series data and determine the most useful features for comparing downsampling algorithms. 

- Survey existing metrics that are used to compare downsampled data representativeness to help inform our evaluation method. 

- Develop comparative visualisations of the common properties of time series data across different downsampling techniques to help communicate the benefits and limitations of particular algorithms and parameters. 

- Conduct user research to understand how decision-makers and data practitioners engage with downsampled time series data.

## Overview of Progress (20%)

By 9th of June, foundational reading conducted, baselined current measures, drafted and sent out UR questions, consent form, invite and list. 

## Project Plan (20%)

Phase 1: By End of JUne: EDA for common properties, schedule UR + impact of downsampling on common properties in comparison to original data. 

Phase 2: By Mid July:  Comparitive viausalisation of downsampling and UR interviews

Phase 3: By end of Jully: UR interviews + wrtie up of UR reflections + poster design

Phase 4: Final write up, iteration of research and visualisations as needed.

Note the plan must be visual and be based on a graphical representation of the project timeline (e.g. a Gantt chart). Tools such as teamgantt.comLinks to an external site. provide strong functionality and a free trial version.

Risks:

- Not enough UR (balance practitioners decision-makers / send invite to as many as possible)

- UR interest, but not scheduled

- Too many properties to create clear common principles for comparison.  (iterate score and approach as needed)

- data availability (use open source, cleaned data from turing). 

- time (part time - scope)

- finding common properties that are sufficiently clear for visuals- take too long / disrupts other project elements (use pre-exiting visuals as prompts in UR?)

## References (10%)

The report should contain an appropriate list of references that you have considered during your research and these should be cited appropriately within your report. These should be formatted appropriately and should be cited within the document.

## Data Management Plan (10%)

A data management plan (DMP) is a written document outlining how you are planning to manage your research data both during and after your research project. The plan should address what types of data will be collected and how the data will be documented, stored, shared and preserved.

## Form (10%)