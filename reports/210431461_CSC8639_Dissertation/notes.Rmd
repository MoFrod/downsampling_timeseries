The UK Government is committed to making data-driven decisions that engender public trust [@data2017; @data2020; @data2021; @data2022]. Data-driven decisions are considered to be "more well-informed" @data2017, effective @data2022, consistent @data2021, and better "at scale" @data2020. Despite this, there is a lack of trust in government use of data @trust. This suggests that public trust in data-driven decisions goes beyond how the "data complies with legal, regulatory and ethical obligations" @data2021. Transparency is needed for the UK public to have "confidence and trust in how data, including personal data, is used" [@data2020; @trust]. 


The UK Government expects that improving real-time access to "correct, timely and trusted data" [@data2022; @data2021] will drive drive better insights and decisions for the UK public. Realising this expectation may, however, prove illusory. Data pipelines (including collection, processing, storage, analysis and visualisation) can obscure or loose important information making it difficult to determine and explain that the data reliably and truthfully reflects the situation a decision-maker is considering. The real-time, high volume data available today makes this even more difficult @TVStore.  


This problem is particularly pertinent to collections of observations obtained through repeated measurements over time @AusBureau, known as time series data. This is one of the data types used in the UK Government @pathway. Indeed, the UK Office for National Statistics *Time series explorer* tool shares 55,612 time series data sets @onstool, suggesting that this data type is readily available for government decision-makers. 


"[W]idely generated by industry and research at an increasing speed" @TVStore, voluminous time series data is putting unprecedented demand on resources [@CatchUp; @IoT]. This is forcing data practitioners to utilise methods, such as aggregation, windowing, and downsampling, that reduce data volumes to align with cost or time limitations, storage capabilities, and sustainability ambitions [@Sveinn; @TVStore; @Shift]. These reduction methods involve discarding data that could result in the further loss of important information for decision-makers.


However, discarding this data is a vital part of making voluminous time series understandable for human observation @Sveinn. Downsampling, for example, reduces "...the number of data points while preserving the overall shape of the time series" @MinMaxLTTB. This allows the human eye to observe only the most valuable data points. Line graphs are an effective and popular method for visualising this data @MinMaxOrdered. Despite effectively conveying the overall shape of the time series data @TimeOrientated, they offer little transparency into which downsampling approaches and parameters best represent the original data. 


This paper posits that such transparency is important for government decision-makers who "mobilise the power of data" @ons to make better decisions for the UK. To make data-driven decisions, decision-makers must trust that the data being considered sufficiently represents the situation they are deciding on. This means trusting which data points are selected, how this data collected and stored, and the capability of data practitioners to understand the quality, insights and limitations of it. 


Supporting data practitioners to determine and communicate whether data being considered by decision-makers reliably and truthfully reflects the situation in question is likely help increase decision-makers trust in data-driven decision-making. 

far less attention has been devoted to the role of transparency in how decision-makers make data-driven decisions

 - motivation
 - what other approaches (have / are there?)
 - Main research question / aim
 - sections of the paper
 
-  HM Government is committed to making more effective use of data to enable better, data-driven decisions and engender public trust in the UK.
- Data-driven decision-making is an act of trust in the data and the data-practitioners.
- At every stage of the data pipeline, there is the opportunity to for subjectivity, uncertainty and risk to arise. 
- high volume time series
- This paper posits that decison-makers are left unaware of the impact of data-practioners' decisions during the data processing pipeline, and that more needs to be done to communicate the impact of these.
- To test this, this paper explores the process of downsampling of time series data - a well established data type government decision-making. 
- It combines analysis of the impact of downsampling with user research to generate insight about how to better communicate the impact of downsampling.

# RELATED WORK

This section provides a comprehensive overview of related work
in the field. (Summarise section)  By doing so, we aim to offer a clear understanding of
the current state-of-the-art and identify the gaps that our work seeks
to address.

Trust

Although easy to grasp intuitively, transparency is hard to define and even harder to realize.@digital_transparency - Merely opening data does not result in digital transparency and might only result in information overload for those wanting to examine such data.

transparency” initiatives become part of an obfuscation process that often uses the rhetoric of placation and diversion @transparency_obfuscation


Ananny and Crawford argue transparency alone can not create accountable systems as simply looking is insufficient. @transparency_lack
Mike Ananny and Kate Crawford (2018) reinforce this point, noting that “the implicit assumption behind calls for transparency is that seeing a phenomenon creates opportunities and obligations to make it accountable and thus to change it” (Ananny and Crawford, 2018: 974, emphasis in original). They describe the promise of transparency as being rooted in the connection between seeing, knowing, and controlling. 

Socially meaningful transparency moves away from meaninfulness in relation to individuals' specific needs to o focus attention on societal needs in terms of what is made transparent, for
whom, how, when and in what ways, and, crucially, who decides. @social_transparency


told that access to information is essential, but without
the tools for turning that access to agency - transparency fallacy - archieving meaningful transparency is difficult @transparency_fallacy

The political valence of data transparency is a critical reminder of the inherently sociopolitical nature of all technologies, including institutional data practices. @political_transparency 

- time series volume

@TVStore - "...mounting demands have emerged for keeping time series data for future analysis [94]. But time series data are generated at a growing speed that is outpacing the increase of computing capabilities [17, 79]. Many application scenarios cannot afford enough computing resources such as storage and network bandwidth to accommodate the processing needs for time series data." pg 84

- time series visualisation


Data-driven decision-making necessitates that time series data is kept for future analysis. 


- characteristics of time series
- downsampling 

Examples of these approaches are set out in the table below:


 Henderson and Fulcher characterised the behaviour and overlaps across prominent time series feature extraction libraries: `hctsa`, `catch22`, `tsfeatures`, `feasts`, `Kats`, `tsfresh`, and `TSFEL` @henderson.


Conclusion: The data pipeline developed by C. H. Lubba et. al could be used to generate other subsets of time series features for distinct tasks in any domain. This is likely to be important for highly specialised tasks or domains.

- trust



- masters thesis
- imputeTS

In statistics this
process of replacing missing values is called imputation.

At the moment imputeTS (Moritz, 2016a) is the only package on CRAN that is solely dedicated to
univariate time series imputation and includes multiple algorithms. Nevertheless, there are some
other packages that include imputation functions as addition to their core package functionality. Most
noteworthy being zoo (Zeileis and Grothendieck, 2005) and forecast (Hyndman, 2016). Both packages
offer also some advanced time series imputation functions. The packages spacetime (Pebesma, 2012),
timeSeries (Rmetrics Core Team et al., 2015) and xts (Ryan and Ulrich, 2014) should also be mentioned,
since they contain some very simple but quick time series imputation methods.

Univariate means there is just one attribute that is observed over time. Which leads to a sequence
of single observations o1
, o2, o3, ... on at successive points t1
, t2, t3, ... tn in time

- Rcatch22

Selecting an appropriate feature-based representation of
time series for a given application can be achieved through systematic comparison
across a comprehensive time-series feature library, such as those in the hctsa toolbox.
However, this approach is computationally expensive and involves evaluating many
similar features, limiting the widespread adoption of feature-based representations of
time series for real-world applications. In this work, we introduce a method to infer
small sets of time-series features that (i) exhibit strong classification performance
across a given collection of time-series problems, and (ii) are minimally redundant.
Applying our method to a set of 93 time-series classification datasets (containing over
147,000 time series) and using a filtered version of the hctsa feature library (4791
features), we introduce a set of 22 CAnonical Time-series CHaracteristics, catch22,
tailored to the dynamics typically encountered in time-series data-mining tasks.

This
dimensionality reduction, from 4791 to 22, is associated with an approximately 1000-
fold reduction in computation time and near linear scaling with time-series length,
despite an average reduction in classification accuracy of just 7%


An ideal starting point for such an exercise is the comprehensive library of
over 7500 features provided in the hctsa toolbox (Fulcher et al. 2013; Fulcher and
Jones 2017).

- visualisation of time series
- turing change point / annotated change

