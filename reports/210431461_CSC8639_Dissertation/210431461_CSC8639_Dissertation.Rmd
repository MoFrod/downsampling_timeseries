---
title: The Explainability of Time Series Downsampling
authors:
  - name: Morgan Frodsham
    department: School of Computing
    affiliation: Newcastle University
    location: Newcastle upon Tyne, UK
    email: M.C.M.Frodsham2@newcastle.ac.uk
abstract: |
  Trusting data-driven decision-making goes beyond demonstrating compliance with legal, regulatory and ethical obligations; decision-makers also need to trust how the data is used. Handling, storing and visualising the volume of data being generated today requires data practitioners to make assumptions and processing choices that remain opaque to decision-makers. Downsampling is an established technique to select a representative subset of time series data that preserves the original data shape while reducing the number of data points in the time series. The research outlined by this paper explores decision-makers' trust in data, data practitioners' experience of communicating data insights to decision-makers and a new visualisation methodology for explaining the impact of downsampling on high-volume time series data. It combines user research insights from interviews with 16 UK Civil Servants with analysis of time series features for 900 imputed time series to identify and visualise the features that are most sensitive to downsampling. This research shows the potential for improving decision-makers' trust in data by helping data practitioners to create transparency in the data processing pipeline, communicate the impact of downsampling, and support conversations about which algorithms or parameters are most appropriate for particular decision-maker use cases.

citation-package: natbib
bibliography: references.bib
csl: ieee-transactions-on-cloud-computing.csl
documentclass: article
output: rticles::arxiv_article
link-citations: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
```

```{r ProjectTemplate, include = FALSE}
library(ProjectTemplate)
setwd("D:/Morgan/Documents/NCL MSc/final_project/downsampling_timeseries")
load.project()
```

# INTRODUCTION

HM Government is committed to making data-driven decisions that engender public trust [@data2017; @data2020; @data2021; @data2022]. Data-driven decisions are considered to be *"more well-informed"* @data2017, effective @data2022, consistent @data2021, and better at scale @data2020. Despite this, there is a lack of trust in government use of data @trust. This suggests that public trust in data-driven decisions goes beyond how the *"data complies with legal, regulatory and ethical obligations"* @data2021. The UK public need to have *"confidence and trust in how data, including personal data, is used"* @data2020, and this requires transparency @trust. 

To make data-driven decisions, it is likely that government decision-makers' need transparency of how data is used to trust the data insights they are presented with. Trust implies trusting which data points are selected, how this data is collected and stored, and the capability of data practitioners to understand the quality, insights and limitations of it. At every stage of the data processing pipeline, data practitioners have the opportunity to communicate the data impact of assumptions and choices they are making. This could support decision-makers in trusting the data informing their decisions while helping decision-makers understand limitations of the data and thresholds at which data insights can be relied upon for each decision.

The research shared in this paper explores this dynamic with a two pronged approach. User research with 16 UK Civil Servants investigates what helps decision-makers trust the data insights they are presented with and how data practitioners assess, and communicate, the trustworthiness of the data. These insights are combined with analysis of how to explain the impact of downsampling algorithms on time series data without detailing extensive statistical evidence. 

Downsampling involves selecting a representative subset of data to preserve its original shape while reducing the number of data points [@datapoint; @MinMaxLTTB]. Time series data is used throughout HM Government @pathway to inform decision-makers across various domains @onstool; it is also widely generated and used by industry and research @TVStore. However, the volume of time series data is continuously increasingly @datapoint, posing significant challenges for handling and visualising this popular data type @TVStore. 

Data practitioners must use methods that reduce the quantity of data (data volume) to align with limitations like processing time, computing costs, storage capabilities, and sustainability ambitions [@Sveinn; @TVStore; @Shift]. Downsampling is an established technique [@downsampling; @sampling] for reducing data volumes. Data reduction is vital for making voluminous time series understandable for human observation @Sveinn and an essential step in many time series database solutions @datapoint. However, little attention has been devoted to how downsampling impacts decision-makers trust in the data. How to communicate the impact of downsampling algorithms on time series data also remains understudied [@Sveinn; @datapoint]. 

Downsampling is an important part of how time series data is used and is a technical issue whose impact, this research posits, is important for decision-makers to understand. Downsampling expands the boundaries of uncertainty and risk for decision-makers as data practitioners may not realise the significance of the data being discarded. For example, it is time and computing resource intensive to transport the terabytes of data generated daily by offshore oil platforms @CISCO without data discarding techniques like downsampling @TVStore. For decision-makers of platform safety and productivity, it is vital to have this information as quickly as possible and understand how the discarded data limits the insights they are considering. It is also important a decision-maker looking at analysis of $CO_2$ levels on an open-plan office floor to decide how many officials can work in the office each day post-Covid. Different downsampling algorithms preserve different data points [@Sveinn; @datapoint; @MinMaxLTTB], which could change the risk threshold for the decision-maker. In complex decisions, such Choices throughout the data pipeline may have cumulative or disproportionately larger consequences later as their future ramifications are unlikely to be fully understood @challenger. It is important, therefore, that data practitioners are able to communicate the impact of choices made throughout the data pipeline.

This paper shares initial insights from user research on decision-makers' trust in how data is used and suggests a visualisation methodology for communicating the impact of downsampling algorithms on time series. This methodology combines user research insights with R packages `imputeTS` @imputeTS_R and `Rcatch22` @catch22_R to identify time series features that are most sensitive to downsampling and visualise the impact of different downsampling algorithms on these features. It is hoped this will improve how data practitioners communicate the impact of downsampling, support conversations about which algorithms or parameters are most appropriate for particular use cases, and increase decision-makers' trust in data by creating transparency in the data processing pipeline. 

# RELATED WORK

This section provides an overview of previous related work to create a clear understanding of the most relevant fields for this research and identify the gaps being addressed by the paper.


**2.1 Data transparency**

Technology transparency, including institutional data practices, is sociopolitical in nature @political_transparency. There is a growing number of researchers reflecting on society's transparency needs *"in terms of what is made transparent, for whom, how, when and in what ways, and, crucially, who decides"* @social_transparency. 

The implicit assumption behind calls for transparency is that *"seeing a phenomenon creates opportunities and obligations to make it accountable"* and, therefore, to change it @transparency_lack. However, without sufficient agency to explore the information being shared, seeing a phenomenon often results in information overload that obfuscates or diverts @transparency_obfuscation. Without agency, transparency is increasingly considered to be a fallacy @transparency_fallacy.

Meaningful transparency is only realised when the information is provided with the tools to transform access into agency [@transparency_fallacy; @transparency_lack]. This suggests that data practitioners simply communicating the assumptions and choices made throughout the data processing pipeline to decision-makers is not likely to create trust in how the data is used. Instead, data practitioners should be encouraged to find tools, such as interactive visualisations @datapoint, that put agency into the hands of decision-makers.

**2.2 Time series visualisation**

Time series data is commonly visualised as a line graph [@Sveinn; @timenotes; @timetuner]. Line graphs help the human eye to observe only the most important data points @Sveinn by conveying the overall shape and complexity of the time series data [@downsampling; @datapoint]. The most effective time series visualisations are, however, interactive [@timenotes; @plotly; @timetuner], turning access into agency @transparency_fallacy by allowing the user to access details on demand. Evaluation of time series visualisation is, therefore, a growing field of research [@Sveinn, @timenotes; @datapoint]. 

However, this growing field of research does not extend to visualisations of choices and assumptions made during data processing pipeline. Indeed, such visualisations are often a side effect of research. This is exemplified by the R package `imputeTS` @imputeTS_R where the impact of imputation choices made by the user is only visualised to support the user through the complete process of replacing missing values in time series [@imputeTS; @missingdata]. The research set out in this paper harnesses the capabilities of `imputeTS` and its 'process' visualisations to help data practitioners communicate the impact of downsampling choices made in the data processing pipeline.

**2.3 Value preserving data aggregation**

Technological innovation has generated an unprecedented amount of time series data and this data volume continues to grow [@data2020; @TVStore; @storage; @CatchUp]. For example, tackling climate change is the UK Government's *"number one international priority"* @IR, yet climate simulations that help inform decision-makers generate tens of terabytes per second [@TVStore; @climate]. Value preserving data aggregation (a subset of downsampling) algorithms play an important role in addressing how this voluminous data is processed, stored @TVStore, and visualised [@Sveinn; @dashql] by minimising the computing resources needed @TVStore, reducing network latency, and improving rendering time [@MinMaxLTTB; @datapoint]. 

Value preserving data aggregation algorithms preserve the original data points that are considered to best represent the original data. This research uses two of the simplest algorithms: *EveryNth* @EveryNth and *Percentage Change* @boxcar. Descriptions of commonly used value preserving data aggregation algorithms are provided in the table below [@datapoint; @EveryNth; @Sveinn; @MinMaxLTTB; @MinMaxOrdered; @M4; @dashql; @boxcar]:

\renewcommand{\arraystretch}{1.5}

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.height=4.1, fig.width=3}
library(kableExtra)

# Manually create a table 
table_content <- matrix(
  c(
    "EveryNth", "Selects every nth data point.",
    "Percentage Change", "Selects every data point beyond a specified percentage of the previous data point",
    "Mode-Median Bucket", "Selects mode or median within equally sized data buckets.",
    "Min-Std-Error-Bucket", "Selects data points with standard error of the estimate (SEE) in linear regression.",
    "MinMax", "Preserves minimum and maximum data points in each data bucket.",
    "M4", "Selects the first and last as well as the minimum and maximum data points",
    "Longest-Line Bucket", "Calculates line length (euclidean distance) between data buckets and selects one point per bucket which forms the longest total
line length through all the buckets.",
    "Largest Triangle Three Bucket (LTTB)", "Calculates the largest triangular surface between the previously selected data point and the average value of the next data bucket to select the highest point.",
    "MinMaxLTTB", "Preselects data points using MinMax before applying Largest Triangle Three Buckets."
  ), nrow = 9, byrow = TRUE
)

# Define the column names
colnames(table_content) <- c("Algorithm", "Description")

# Create the table with the title "Example Downsampling Algorithms"
knitr::kable(table_content, caption = "Example Downsampling Algorithms") %>%
  kable_classic_2(full_width = F) %>%
    column_spec(1, width = "1.8in") %>%
  column_spec(2, width = "3.8in") %>%
  kable_styling("striped", latex_options = "HOLD_position")
  

```

Data practitioners have recently made advances in the performance and evaluation of downsampling approaches [@downsampling; @sampling; @EveryNth; @MinMaxOrdered; @MinMaxLTTB; @dashql; @datapoint; @plotly; @boxcar]. These advances focus on the effectiveness of the algorithm in delivering downsampled data that represents the original data as accurately as possible. This is a vital part of enabling and improving data-driven decision-making, but is focused on supporting data practitioners in their data analysis. Instead, the research set out in this paper aims to support data practitioners to communicate the impact of their downsampling choices for decision-makers.

**2.4 Time series feature analysis**

The increasing sizes of modern time series data sets have also generated new research into the characteristics of time series @catch22. These characteristics are often used to identify features that enable efficient clustering and classification of time series data, especially for machine learning. A comprehensive library of such features is the highly comparative time series analysis `hctsa` toolbox. This shares the 4791 high-performing features after computationally comparing thousands of characteristics from across scientific time series analysis literature @fulcher2017. 

Utilising such a library, however, is computationally expensive @catch22. C. H. Lubba et. al have attempted to address this by identifying a subset of 22 features (as well as mean and standard deviation spread) that are tailored to time series data mining tasks [@bagnall; @catch22], which are shared in the `Rcatch22` R package. Although further research is needed to evaluate the relative performance of different feature sets on different types of problems, the 'catch22' features performs well against other feature libraries across 800 diverse real-world and model-generated time series @henderson. 

Features used to classify time series data could provide a common framework by which to consistently compare different downsampling algorithms and parameters. The research set out in this paper utilises the `Rcatch22` subset of features to explore impact of downsampling and create a visual tool for explaining this impact.

# USER RESEARCH

Interviews with 16 UK Civil Servants (nine decision-makers and seven data practitioners) highlight the importance of transparency in enabling data-driven decision-making across government. This section outlines how the user research is designed and the key insights it generated. 

**3.1 Design**

User research produced over 16 hours of recorded and transcribed interviews with decision-makers and data practitioners (these outputs are confirmed in Annex A). These interviews are hosted virtually on MS Teams, instead of in-person, to enable recording and transcription; this creates the potential to move beyond qualitative thematic analysis and implement alternative techniques, such as natural language processing. Interviews are selected over surveys because concepts like transparency and trust are complex and nuanced. One-to-one interviews are selected instead of focus groups to minimise authority bias and potential groupthink. 

Like many interview studies @futzing, the participants for this user research are selected by contacting relevant individuals in the researcher's professional network. This introduces bias as the sample of participants is neither representative nor drawn from the overall population. This could engender confirmation, false consensus and group think biases as these participants are likely to operate in the same professional bubble, which would skew the insights generated by this user research. To mitigate further biases, two interview scripts used; one for all decision-makers and the other for all data practitioners. However, it is important to note that follow-up questions vary to ease the flow of each interview and help participants to share further detail in their responses.  

Qualitative analysis is conducted on the content of each interview to understand the themes and identify quotes that reflect the participant's sentiments. This is combined with natural language processing keyword extraction. The python libraries `docx` and `nltk` are used to import and clean the interviews text data; capitalisation, 'stopwords', and words with three or fewer letters are removed. Interviews are anonymised as the time stamps, interviewer name and participant name are removed  @jono; text data from the interviewer is also removed so that natural language processing is only conducted on participant responses. Limited natural language processing is then applied by tokenising the remaining words and counting the number of each unique word. These word counts alongside the qualitative thematic analysis inform concept maps @GPT which aim to show the key themes and words. 

**3.2 Insights**

Transparency of data provenance, collection, and analysis pipeline is clearly desired by decision-makers to understand the reliability, repeatability, and quality of the data they're presented with. All inerviewed decision-makers and data-practitioners shared that this is best communicated through engaging conversations and interactive visualisations. Both decision-makers and data practitioners emphasised the importance of communicating the story behind data, its assumptions, and how the data changes through time. Everyone interviewed acknowledged that this is difficult to achieve and how to improve this is a topic of live debate. 

Insights from this user research indicate that decision-makers desire a spectrum of transparency for decisions. This spectrum depends on the question being addressed by the data: it's complexity, importance and potential impact. To trust the data informing data-driven decisions, decision-makers and data practitioners repeatedly request transparency of the data context, sourc,e and limitations as well as an assessment of overall confidence in the data.  

Quotes from the interviews highlight that decision-makers find it difficult to develop the correct views of *"how much they can trust the data"*. They address their desire for more emphasis on the *"metadata and how to appropriately describe the level of uncertainty and provenance of the data"* to help ensure the data is being applied to the context it was collected for. They share the importance of understanding if *"there is anything that is baked into what is being provided"* in order to trust the data and, how the source of this trust changes as data practitioners become decision-makers because they rarely see the raw data and so *"have to have an understanding of what preprocessing has been done, because it almost always has been [done]"*. Decision-makers share that visualisations are important for conveying *"specific insights that simply looking at numbers can't get over"* and emphasise that if analysts *"clearly explain what the limitations are and rigorously present that, then I'll trust [the analysis] more"*. Quotes from data practitioner interviews are shared in Annex B.

Figure 1 below shares example concept maps outlining the key themes from two anonymised decision-maker and data-practitioner interviews. All the interview concept maps are shared in Annex C; only summative content from these interviews is presented in compliance with the consent forms shared with the user research participants.

```{r ConceptMapExample, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4.1, fig.with=3}
library(gridExtra)
library(grid)
library(magick)



# Define custom margins (top, right, bottom, left)
custom_margins <- unit(c(0, 0, -0.5, 0), "lines")

# Define the paths to maps
image1 <- "D:/Morgan/Documents/NCL MSc/final_project/mofrod_project/graphs/userresearch/Decision-maker 9a_ Concept Map.png"
image2 <- "D:/Morgan/Documents/NCL MSc/final_project/mofrod_project/graphs/userresearch/Data Practitioner 7a_ Concept Map.png"

# Read the images
img1 <- image_read(image1)
img2 <- image_read(image2)

# Convert images to raster
img1_raster <- as.raster(img1)
img2_raster <- as.raster(img2)

# Convert raster to grobs
img1_grob <- rasterGrob(img1_raster)
img2_grob <- rasterGrob(img2_raster)

# Create a text grob for the title with custom font settings
title_grob <- textGrob(
  "Figure 1: Example Concept Maps",
  x = 0.16, 
  y= 0.5,
  gp = gpar(fontfamily = "serif", fontsize = 11)
)

# Arrange the images side by side
grid.arrange(img1_grob, img2_grob, ncol = 2, heights = c(7, 2.3), top=title_grob)

```
\vspace{-3.3cm}

The findings from this user research indicate that decision-makers need transparency of how the data is used to trust the insights informing data-driven decisions. This suggests that endeavours to create greater transparency in data pipelines, such as the research outlined in the subsequent sections of this paper, are worthy of further study. It also suggests that evaluating the objectivity and quality of data sources as well as mechanisms to develop common descriptions of data insight uncertainty or confidence-levels are potential avenues for data science to have greater policy impact.

It is important to reiterate that the findings of this user research are limited and, potentially, biased. Given the consistency of insights from the user research participants, it is clear that further interviews with a wider, random subset of the decision-maker and data practitioner populations in UK Government would be beneficial.

# ANALYSIS METHODOLOGY

This section outlines the analysis design undertaken to identify and communicate the impact of downsampling algorithms on time series data without extensive statistical evidence. 

**4.1 Data**

The eight Alan Turing Institute `Annotated Change`  @ATIChangePoint synthetic time series are selected for initial analysis. These are selected because they are cleaned time series from a reputable institution with known change points that are designed to provide examples of different types of time series. However, there is an inherent limitation in using synthetic time series for analysis because the data has been generated for a different purpose. These synthetic time series ('100', '200', '300', '400', '500', '600', '700', '800') are used in demonstrations to support change point annotators annotate real-world data sets for the `Turing Change Point Data set` @ATIChangePoint. The eighth `Annotated Change` synthetic time series ('800') is multivariate and thus split in two ('800A' and '800B') to create nine synthetic time series for this research and enable better comparison. The nine time series are visualised in Annex D.

**4.2 Downsampling**

These synthetic time series are imported into `RStudio` from `JSON` scripts and two downsampling algorithms are applied to all with the `Jettison MVP Code` @Jettison. This enables initial insights into the impacts of downsampling with minimally complex code. The two algorithms used by the `Jettison MVP Code` @Jettison are *EveryNth*, which selects every other data point, and *Percentage Change*, which selects every data point that has greater than 1% difference between the last transmitted and newest values. 

The `Jettison MVP Code` @Jettison applies these algorithms iteratively across each of the nine synthetic time series. This creates parameters 1 to 50 for each downsampling algorithm and `Annotated Change` time series. There are now 900 time series for investigation, 450 for each downsampling algorithm.

The R package `imputeTS` @imputeTS_R is applied to all 900 time series to obtain the remaining data volume after downsampling, number of missing values (NAs), and number of gaps in the time series. This is important to understand the effect of each downsampling algorithm. Initial analysis highlights that the effect of each downsampling algorithm is unevenly distributed across the 50 parameters; for example, *EveryNth* discards data more quickly than *Percentage Change* so comparing the time series for each parameter is insufficient. The remaining data volume, therefore, provides a common variable that enables like-for-like comparison between the downsampling algorithms and their impact.

The missing values for each of the 900 time series are replaced by the `imputeTS` linear interpolation function. This is a simple imputation algorithm that returns each time series to its original length and is a commonly used approach to present line graphs @compressiontech. Returning the time series to their original length enables a like-for-like comparison of downsampling impact. For each `Annotated Change` synthetic time series, there is now an imputed time series for parameters 1 to 50 and each downsampling algorithm.

**4.3 Features**

The `Rcatch22` package @catch22_R enables the computation of a value for each of the `catch22` time series features @catch22. Values for the 24 time series features are calculated for each of the nine original time series and 900 imputed time series. The difference between the original and imputed feature values is calculated and that difference scaled, dividing by the standard deviation with the `scale()` R function, to accurately compare how the downsampling algorithms impact these `catch22` features. The coefficient of variation across the 900 imputed time series for each `catch22` feature is also calculated to indicate and compare the sensitivity of each feature to downsampling. The `catch22` features are considered to be more sensitive if their coefficient of variation across the 900 imputed time series is higher. 

It is important to note that the `catch22` time series features are selected to reflect characteristics that help cluster and classify different time series, not selected for their impact on downsampling. This feature analysis design could be taken further by investigating whether features sensitive to downsampling are equally applicable to all time series types. 

**4.4 Visualisation**

Combining `imputeTS` @imputeTS_R and `Rcatch22` @catch22_R in this way is unstudied. Initial visual analysis to observe the impact of downsampling on `catch22` features is conducted by using bar and line graphs alongside heatmaps. All the decision-makers interviewed as part of the user research stressed the importance of visualisation in data transparency and data-driven decision-making. In line with this, different approaches to visualising the impact of downsampling are tried to explore how the impact of downsampling could best be visually communicated by data practitioners. The visualisations created by this research are under-developed and would benefit from usability research to ensure they meet the aim of this research.

# RESULTS AND EVALUATION

\vspace{-0.4cm}

This section shares and discusses the results of this research, identifying their limitations and potential avenues for further investigation. These results are presented in three themes: downsampling sensitivity, downsampling impact, and feature variation.

\newpage
**5.1 Downsampling sensitivity**

Seven `catch22` features show a wider dispersion across the 900 imputed time series than the other 15 features. These seven features are considered to be more sensitive to the impacts of downsampling. To demonstrate the sensitivity of these seven features, features with the highest absolute coefficient of variation values across all the imputed time series are visualised in the bar graph below. The absolute coefficient of variation values for all 24 features is shared in Annex E.  

```{r CombinedSensitivity, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3.5, fig.with=3}
# Reshape sensitivity subset data to a long format
cv_subset_sensitivity_joined_long <- cv_subset_sensitivity_joined %>%
  pivot_longer(cols = c(everyNthSensitivity, PercentageChangeSensitivity), 
               names_to = "Method", 
               values_to = "CoefficientofVariation")


# Change the Method into a factor and set the levels
cv_subset_sensitivity_joined_long$Method <- factor(cv_subset_sensitivity_joined_long$Method, levels = c("everyNthSensitivity", "PercentageChangeSensitivity"))

# Recode the names of the Rcatch22 features
cv_subset_sensitivity_joined_long$names <- recode(cv_subset_sensitivity_joined_long$names, 
                                   'CO_f1ecac' = "Autocorr_ApproxScale",
                                   'CO_FirstMin_ac' = "Autocor_FirstMinimum",
                                   'SB_BinaryStats_mean_longstretch1' = "LongestPeriod_AboveAverage",
                                   'PD_PeriodicityWang_th0_01' = "Autocorr_FirstPeak",
                                   'DN_Mean' = 'Mean',
                                   'DN_Spread_Std' = 'Spread',
                                   'CO_Embed2_Dist_tau_d_expfit_meandiff' = "DistributionExponentialFit_MAE",
                                   'IN_AutoMutualInfoStats_40_gaussian_fmmi' = "Autocorr_Automutual",
                                   'SB_BinaryStats_diff_longstretch0' = "LongestPeriod_SuccessiveDecreases")

# Recode the names of the Methods
cv_subset_sensitivity_joined_long$Method <- recode(cv_subset_sensitivity_joined_long$Method,  
                                                "PercentageChangeSensitivity" = "Percentage Change", 
                                                "everyNthSensitivity" = "EveryNth")

# Arrange in order
cv_subset_sensitivity_joined_long <- cv_subset_sensitivity_joined_long %>%
  arrange(CoefficientofVariation)

# Convert "names" to a factor
cv_subset_sensitivity_joined_long$names <- factor(cv_subset_sensitivity_joined_long$names, levels = unique(cv_subset_sensitivity_joined_long$names))

# Create a bar plot
ggplot(cv_subset_sensitivity_joined_long, aes(x = CoefficientofVariation, y = names, fill = Method)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  theme_minimal() +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -4, size=11), legend.position = "top", legend.justification = c(-1.05, 1)) +
  labs(x = "Coefficient of Variation", y = "Catch22 Feature", fill = "Method", title = "Figure 2: Coefficient of Variation for Most Sensitive Features") +
  scale_fill_brewer(palette = "Paired") +
  scale_x_continuous(limits = c(0, NA))
```

Figure 2 suggests that the *Percentage Change* algorithm impacts the `catch22` time series features more than the algorithm. This is surprising as the *EveryNth* algorithm discards more data, more quickly, in the 50 parameters of downsampling. The sensitivity of these features to the *Percentage Change* is worthy of further statistical analysis to understand why it impacts the feature's coefficient of variation values more. It would also be beneficial to investigate the impact of *Percentage Change* further by creating downsampling parameters beyond 50 until the same volume of data is discarded by both algorithms.

The coefficient of variation values for the seven most sensitive `catch22` features, rounded to to two deciamal points, are presented in table 2. 

\vspace{-0.2cm}

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results = 'asis'}

# Create a combined dataframe of all sensitivity
cv_subset_sensitivity_joined <- cv_subset_sensitivity_joined %>%
  rename(Feature = names) %>%
  mutate(
    everyNth = round(everyNthSensitivity, 2),
    `Percentage Change` = round(PercentageChangeSensitivity, 2),
    Combined = round(CombinedSensitivity, 2)
  ) %>%
  select(Feature, everyNth, `Percentage Change`, Combined) %>%
  head(7)

# Recode the names of the Rcatch22 features
cv_subset_sensitivity_joined$Feature <- recode(cv_subset_sensitivity_joined$Feature, 
                            'CO_f1ecac' = "Autocorr_ApproxScale",
                            'CO_FirstMin_ac' = "Autocorr_FirstMinimum",
                            'SB_BinaryStats_mean_longstretch1' = "LongestPeriod_AboveAverage",
                            'PD_PeriodicityWang_th0_01' = "Autocorr_FirstPeak",
                            'CO_Embed2_Dist_tau_d_expfit_meandiff' = "DistributionExponentialFit_MAE",
                            'IN_AutoMutualInfoStats_40_gaussian_fmmi' = "Autocorr_Automutual",
                            'SB_BinaryStats_diff_longstretch0' = "LongestPeriod_Decreases")


knitr::kable((cv_subset_sensitivity_joined), caption = "Coefficient of Variation for the Seven Most Sensitive Catch22 Features") %>%
  kable_classic_2(full_width = F) %>%
  kable_styling("striped", latex_options = "HOLD_position")
```

The combined absolute coefficient of variation values are used to identify the seven `catch22` features that appear to be most sensitive. Table 2 shows that the mean absolute error of an exponential fit to a probability distribution @feature_book (`DistributionExponentialFit_MAE` or `CO_Embed2_Dist_tau_d_expfit_meandiff`) has the highest combined sensitivity. The longest sequence of successive steps that decrease (`LongestPeriod_Decrease` or `SB_BinaryStats_diff_longstretch0`) is most sensitive to the the *Percentage Change* algorithm. The `catch22` feature measuring the first minimum of the autocorrelation function (`Autocorr_FirstMinimum` or `CO_FirstMin_ac`) is most sensitive to the *EveryNth* algorithm.

These results indicate that it may be possible to communicate the impact of downsampling via the sensitivity of the `catch22` features. This analysis could be taken further by examining whether the coefficient of variation is the best measure of sensitivity and investigating whether the changes in the `catch22` features is directly caused by the impact of downsampling.

**5.2 Downsampling Impact**

Identifying that seven `catch22` features appear to be more sensitive to the impacts of downsampling enables data practitioners to evaluate and communicate the impact of different downsampling algorithms without deep statistical analysis. The difference between the values of `catch22` features for the nine original time series and 900 imputes time series is used to measure the impact of downsampling. Figure 3 below visualises this difference scaled (divided by the standard deviation) for the seven most sensitive `catch22` features across the 900 imputed time series and each of the 50 parameters. 

```{r Heatmap_param, echo=FALSE, message=FALSE, warning=FALSE, fig.height=5, fig.with=3}
# Recode the names of the Methods
filtered_df$method <- recode(filtered_df$method,  
                                                "PercentageChange" = "Percentage Change")
# Recode the names of the Rcatch22 features
filtered_df$names <- recode(filtered_df$names, 
                            'CO_f1ecac' = "Autocorr_ApproxScale",
                            'CO_FirstMin_ac' = "Autocorr_FirstMinimum",
                            'SB_BinaryStats_mean_longstretch1' = "LongestPeriod_AboveAverage",
                            'PD_PeriodicityWang_th0_01' = "Autocorr_FirstPeak",
                            'DN_Mean' = 'Mean',
                            'DN_HistogramMode_10' = "HistogramBin10",
                            'DN_Spread_Std' = 'Spread',
                            'CO_Embed2_Dist_tau_d_expfit_meandiff' = "DistributionExponentialFit_MAE",
                            'IN_AutoMutualInfoStats_40_gaussian_fmmi' = "Autocorr_Automutual",
                            'SB_BinaryStats_diff_longstretch0' = "LongestPeriod_Decreases")

# Arrange in alphabetical order
filtered_df <- filtered_df %>%
  arrange(desc(names))

# Convert "names" to a factor
filtered_df$names <- factor(filtered_df$names, levels = unique(filtered_df$names))

# Create the heatmap showing scaled difference for most sensitive features across parameter, one per method.
ggplot(filtered_df, aes(x = as.numeric(param), y = names, fill = scaled)) +
  geom_tile() +
  scale_fill_gradient2(low = "#084594", high = "#eff3ff", mid = "#c6dbef",
                       midpoint = 0, limits = range(joined_df$scaled)) +
  theme_minimal() +
  labs(x = "Parameter", y = "Features", fill = "Scaled Difference", title = "Figure 3: Scaled Difference of Sensitive Features by Parameter") +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -3.5, size=11), legend.position = "top", legend.justification = c(-0.98, 1)) +
  facet_wrap(~ method, ncol = 1)

```

Figure 3 suggests that the approximate scale of autocorrelation (`Autocorr_ApproxScale` or `CO_f1ecac`) and the longest sequence of successive values greater than the mean (`LongestPeriod_AboveAverage` or `SB_BinaryStats_mean_longstretch1`) are the first `catch22` features to be impacted by the downsampling algorithms. Interestingly, Figure 3 also demonstrates that the features values impacted by both downsampling algorithms tend to increase in comparison the original values. This warrants further investigation. 

However, the parameters prevent a like-for-like comparison of the two downsampling algorithms as *EveryNth* discards data more quickly than *Percentage Change*. Figure 4 below also visualises the scaled difference between the values of the original `catch22` features and the imputed time series, but across the volume of data retained by the downsampling algorithms before `imputeTS` @imputeTS_R imputes the missing values.

```{r Heatmap_vol, echo=FALSE, message=FALSE, warning=FALSE, fig.height=5, fig.with=3}
# Create the heatmap showing scaled difference for most sensitive features across volume, one per method.
ggplot(filtered_df, aes(x = as.numeric(vol), y = names, fill = scaled)) +
  geom_tile() +
  scale_fill_gradient2(low = "#084594", high = "#eff3ff", mid = "#c6dbef",
                       midpoint = 0, limits = range(joined_df$scaled)) +
  scale_x_reverse() +
  theme_minimal() +
  labs(x = "Volume of Data", y = "Catch22 Feature", fill = "Scaled Difference", title = "Figure 4: Scaled Difference of Sensitive Features by Volume") +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -2.8, size=11), legend.position = "top", legend.justification = c(-0.98, 1)) +
  facet_wrap(~ method, ncol = 1)
```

Figure 4 demonstrates that data volume retained by the downsampling algorithms creates acute differences, especially when fewer than 20 data points remained after *EveryNth* is applied. However we cannot conclude that thre is a significant difference between the algorithms over and above the volume of data remaining without further investigation. Interestingly, the impact of *Percentage Change* appears to be inconsistent; there are some retained data volumes where `Autocorr_ApproxScale` and `LongestPeriod_AboveAverage` do not appear to be impacted by *Percentage Change* even though larger retained data volumes appear to be impacted. This also warrants further investigation. 

Downsampling can perturb the statistical properties @ATIChangePoint and visual perception @graphsampling of the data. `Catch22` features are statistical properties of time series. It is likely that some of these statistical properties will be more or less affected by downsampling because downsampling algorithms preserve different data points from the original time series. A full table of the statistical properties represented by each `catch22` feature is shared in Annex F.

It is not possible, however, to understand the impact of both downsampling algorithms from this visualisation. The *Percentage Change* algorithm needs to be applied for more than 50 parameters so that it discards volumes of data comparable to the *EveryNth* algorithm. Despite this, the heatmap visualisation of downsampling impact holds potential; Figure 4 suggests that, with the same volume of data, the downsampling algorithms impact the `catch22` features differently.  

**5.3 Feature Variation**

The impact of both downsampling algorithms on the most sensitive features appears to vary across different time series types. This is exemplified by the `catch22` feature `LongestPeriod_Decreases` (`SB_BinaryStats_diff_longstretch0`), which *"calculates the longest sequence of successive steps in the time series that decrease"* @feature_book. The feature has the highest coefficient of variation across the 900 time series; the line graphs in Figure 5 below share the differences between the `catch22` feature value of the original time series and the 50 time series imputed from these after downsampling; these differences are scaled for better comparisons. The title of each line graph refers to the original nine synthetic time series (for example, '100', '200', and '300').

```{r LongestDecreases, echo=FALSE, message=FALSE, warning=FALSE, fig.height=5, fig.with=3}
# Recode the names of the datasets
filtered_df$dataset <- recode(filtered_df$dataset,
                              "df100" = "100", 
                              "df200" = "200", 
                              "df300" = "300", 
                              "df400" = "400", 
                              "df500" = "500", 
                              "df600" = "600", 
                              "df700" = "700", 
                              "df800a" = "800A", 
                              "df800b" = "800B")

# Create the scaled difference line graph for SB_BinaryStats_diff_longstretch0
ggplot(filtered_df %>%
         filter(names == "LongestPeriod_Decreases"), 
       aes(x = as.numeric(vol), y = scaled, color = method)) +
  scale_color_brewer(palette = "Paired") +
  geom_line(linewidth = 0.5) +
  scale_x_reverse() +
  theme_minimal() +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -0.6, size=11), legend.position = "top", legend.justification = c(-0.04, 1)) +
  labs(x = "Volume of Data", y = "Scaled Difference", color = "Method", title = "Figure 5: Variation of the Longest Sequence of Successive Steps that Decrease") +
  facet_wrap(~ dataset, ncol = 3)

```

Figure 5 presents the impact of discarding data for both downsampling algorithms. The volume of data retained by the *Percentage Change* algorithm varies by the type of time series whereas the volume of data retained by the *EveryNth* algorithm trends towards zero. The line graphs presented in Figure 5 highlight how different downsampling algorithms may more or less preserve the integrity of different time series. For example, time series ‘300’ and ‘500’ appear to be more impacted by the Percentage Change algorithm than other time series. How the most sensitive `catch22` time series features vary across the different time series types is shared in Annex G.

This variation of downsampling algorithms across time series types causes the author to question whether the downsampling sensitivity of `catch22` features may vary across time series type too. An example of this dynamic is visualised by the line graphs in Figure 6 on the next page, which plot the scaled difference of the six most sensitive `catch22` features for time series '500'. Time series '500' was selected as more data is discarded by the *Percentage Change* algorithm, offering a better comparison.

Figure 6 highlights that, although `LongestPeriod_Decrease` is identified as the most sensitive across both downsampling algorithms, `Autocorr_Automutual` (`IN_AutoMutualInfoStats_40_gaussian_fmmi`) might better indicate the impact for time series '500'. `Autocorr_Automutual` is the minimum of the automutual information function; this `catch22` feature outputs a measure of *"autocorrelation in the time series, as the minimum of the automutual information function"* @feature_book. `Autocorr_FirstMinimum` (`CO_FirstMin_ac`) and `Autocorr_FirstPeak` (`PD_PeriodicityWang_th0_01`) also seem to show early impacts of the *Percentage Change* algorithm.

```{r Catch22Variation, echo=FALSE, message=FALSE, warning=FALSE, fig.height=5, fig.with=3}
# Recode the names of the methods
subset_filtered_df$method <- recode(subset_filtered_df$method,  
                                                "PercentageChange" = "Percentage Change")
# Recode the names of the Rcatch22 features
subset_filtered_df$names <- recode(subset_filtered_df$names, 
                            'CO_f1ecac' = "Autocorr_ApproxScale",
                            'CO_FirstMin_ac' = "Autocorr_FirstMinimum",
                            'SB_BinaryStats_mean_longstretch1' = "LongestPeriod_AboveAverage",
                            'PD_PeriodicityWang_th0_01' = "Autocorr_FirstPeak",
                            'CO_Embed2_Dist_tau_d_expfit_meandiff' = "DistributionExponentialFit_MAE",
                            'IN_AutoMutualInfoStats_40_gaussian_fmmi' = "Autocorr_Automutual",
                            'SB_BinaryStats_diff_longstretch0' = "LongestPeriod_Decreases")

# Arrange in alphabetical order
subset_filtered_df <- subset_filtered_df %>%
  arrange(names)

# Convert "names" to a factor
subset_filtered_df$names <- factor(subset_filtered_df$names, levels = unique(subset_filtered_df$names))

# Recode the names of the datasets
subset_filtered_df$dataset <- recode(subset_filtered_df$dataset,
                              "df100" = "100", 
                              "df200" = "200", 
                              "df300" = "300", 
                              "df400" = "400", 
                              "df500" = "500", 
                              "df600" = "600", 
                              "df700" = "700", 
                              "df800a" = "800A", 
                              "df800b" = "800B")

# Create the difference line graph for 500
ggplot(subset_filtered_df %>%
         filter(dataset == "500"), 
       aes(x = as.numeric(vol), y = scaled, color = method)) +
  scale_color_brewer(palette = "Paired") +
  geom_line(linewidth = 0.5) +
  scale_x_reverse() +
  theme_minimal() +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -0.21, size=11), legend.position = "top", legend.justification = c(-0.04, 1)) +
  labs(x = "Volume of Data", y = "Scaled Difference", color = "Method", title = "Figure 6: Variation of Catch22 Features for Time Series 500") +
  facet_wrap(~ names, ncol = 3)

```

Interestingly, Figure 6 shows that the *Percentage Change* algorithm has the opposite impact on `LongestPeriod_AboveAverage` (`SB_BinaryStats_mean_longstretch1`) than the *EveryNth* algorithm.

A comparison of the most sensitive features for each time series type is shared in Annex H. It would be beneficial to explore this further as this variation may offer data practitioners a method for identifying subsets of features that best indicate the preservation of different time series.

# CONCLUSION

Increasing volumes of time series data are being widely generated and used by industry and research @TVStore. Downsampling is a vital data processing technique for reducing this volume, addressing limitations like processing time, computing costs, storage capabilities and sustainability ambitions [@Sveinn; @TVStore; @Shift]. However, downsampling can perturb the statistical properties @ATIChangePoint and visual perception @graphsampling of time series, potentially impacting the insights that inform data-driven decisions.

Interviews with 16 UK Civil Servants (nine decision-makers and seven data practitioners) highlight the importance of transparency in enabling data-driven decision-making across government. Decision-makers shared that there is a spectrum of transparency desired for the data processing pipeline, which depends on the question being addressed by the data; it's complexity, importance and potential impact. To trust the data informing data-driven decisions, decision-makers and data practitioners repeatedly requested transparency of the data context, source and limitations as well as an assessment of overall confidence in the data. This user research indicates that decision-makers need transparency of how the data is used to trust the insights informing data-driven decisions.

There are seven `catch22` time series features @catch22 that appear to be more sensitive to the impacts of downsampling. `Catch22` features are statistical properties of time series so they are more or less affected by downsampling depending on which data points are preserved from the original time series. The impact of the *everyNth* and *Percentage Change* downsampling algorithms on the most sensitive features appears to vary across different time series types. The downsampling sensitivity of `catch22` features also appears to vary across time series. These results suggest that different subsets of `catch22` features could be selected to best indicate the preservation of time series after downsampling. Tailored subsets of features enable data practitioners to design and build interactive visualisations for decision-makers to explore the limitations of the data they are considering.

The varying sensitivity of `catch22` features to different downsampling algorithms is previously unstudied. This research offers a new visualisation methodology for data practitioners to communicate the impact of downsampling on different decisions and create meaningful transparency about the downsampling choices they are making throughout the data processing pipeline. This could help decision-makers to trust the data informing their decisions, supporting decision-makers to understand the limitations of the data available and the thresholds at which the data insights can or cannot be relied upon for each decision. 

# FUTURE WORK

There are many avenues for future work to better understand the findings, and address the limitations, of this research. 

**6.1 User research**

Further user with a wider, more representative group of decision-makers and data practitioner is needed to develop statistically significant findings for the UK Government. alongside this, additional analysis on the transcripts and recordings of the user research would improve the analysis. For example, using other natural language processing techniques, such as topic modelling, to better identify the connections between the key themes identified. Usability research with decision-makers and data practitioners would also beneficial. Usability would contribute to the refinement of visualisations so that they better communicate the impact of downsampling on different time series. How these visualisations perturb the decision-makers' and data practitioners' perception of downsampling impact is an important avenue of further investigation @graphsampling.

**6.2 Downsampling Sensitivity**

It would be beneficial to use the data pipeline developed by C. H. Lubba et. al @catch22 to generate other subsets of time series features for distinct tasks in different domains. Examining the downsampling sensitivity of new subsets of `catch22` features is likely to generate the information needed to understand why certain features are more sensitive to downsampling across different types of time series. It could also be interesting to explore the impact of data discarding techniques, such as downsampling, through classifications of missing data @missingdata. Such classifications may offer alternative features for evaluating and visualising the impact of discarding data.

**6.3 Downsampling Impact**

Further iterations of downsampling on the synthetic time series used in this research is an important step to address the limitation of this research. These iterations will enable a better comparison of the *EveryNth* and *Percentage Change*. Equally, repeating the analysis methodology of this research for other downsampling algorithms, such as LTTB [@Sveinn; @MinMaxLTTB; @MinMaxOrdered], as well as applying utilising the real-world time series shared by the `Alan Turing Change Point Dataset` @ATIChangePoint set will allow a deeper investigation into the impact of downsampling. These avenues of future work are also vital for generating the information needed to fully realise the ambitions of this research - developing comparative and interactive visualisations to better communicate the impact of downsampling on time series. These visualisations should also be evaluated with users through usability research @graphsampling.

**6.4 Feature Variation**

The future work for understanding downsampling sensitivity and impact could be combined to develop thresholds and benchmarks for different time series. This avenue for further investigation could enable the most relevant and sensitive time series features to be embedded for proactive downsampling that reduces the demand on computing resources. For example, embedding the most sensitive features in smart sensors monitoring a particular type of time series would enable downsampling at the point of collection, reducing demand on computing resources for storage. Such tailoring of downsampling could also facilitate more nuanced discussions between decision-makers and data practitioners on the thresholds of downsampling for different decision types to ensure that insights from downsampled data are used within the bounds of their limitations.

\newpage
  
# Annex A: Acknowledgement of User Research

In line with the signed consent forms, the user research transcripts and unprocessed findings cannot be shared beyond the researcher and their supervisor. Figure 7 shares the supervisor's email confirmation acknowledging the user research conducted. 

```{r, echo=FALSE, out.width="100%"}

# Define the paths to maps
image3 <- "D:/Morgan/Documents/NCL MSc/final_project/mofrod_project/docs/Supervisor_Confirmation.png"

# Read the images
img3 <- image_read(image3)

# Convert images to raster
img3_raster <- as.raster(img3)

# Convert raster to grobs
img3_grob <- rasterGrob(img3_raster)

# Create a text grob for the title with custom font settings
title_grob3 <- textGrob(
  "Figure 7: User Research Confirmation from Supervisor",
  x = 0.26, 
  y= 0.1,
  gp = gpar(fontfamily = "serif", fontsize = 11)
)

# Arrange the images side by side
grid.arrange(img3_grob, top=title_grob3)

```
\newpage

# Annex B: Data Pracitioner Quotes

Interesting (anonymised) quotes that highlight the sentiment of data practitioners from user research interviews are shared below:

- *"You come at it from a particular viewpoint with your particular expertise, somebody with different background, somebody with different expertise might come at the problem in a different way."*

- *"...my impression is that a lot of people make decisions from data without actually understanding what that data represents or simultaneously, and what the theoretical assumptions on our data and data collection is."*

- *"...getting the point across of how downsampling and other techniques effects that data getting that balance right of explaining the assumptions and explaining what what's been done without getting in the technical details..."*

- *"I would probably think about the impact of the decision and that would inform me about whether I'm comfortable making a decision on that with that data."*

- *"...we have to understand what things you may think of the data tells you, but it doesn't actually tell you because that's where you often get caught out."*

- *"...making the assumption of what people how people will think the data will be interpreted versus how you can interpret the data is a really, really useful because you go, OK, people gonna want to say this about this data, but this is what actually sets."*

- *"...if you have done the due diligence, generally decision makers trust you to do the due diligence."* 

- *"The best that you can say in general is that you have to understand what the information requirement is and what the information processing is that happens, and then you can make an estimate of the effect that downsampling, or indeed any information reduction technique has on the on the decision."*

- *"...ensure that people are thinking about what the processes that they are going through in order to make the decision, not so, not just taking the recommendation but actually understanding how it's got there and where the data came from and how that was achieved."*

- *"I think [helping decision-makers trust data] is quite dependent on the personality of who you're dealing with at the time and what they're willing to kind of expose of their own understanding and lack of understanding or motivation."*

- *"Data is collected for a purpose then being used for another purpose and they might not exactly map on top of one another, which I think is quite challenging. Often we end up getting data that was collected by somebody else for something else, and we've got to shoehorn it into what we are interested in."*

- *"I haven't engaged with stakeholders on a level like that to understand whether they are aware of the impacts of decisions like that on any end results and how much of a difference that can make... but it is very much a stakeholder problem because it impacts what you will get out of it."*

\newpage

# Annex C: User Research Concept Maps

\vspace{-0.4cm}

Key word counts alongside the qualitative thematic analysis inform concept maps which visualise the key themes and key words. The concept maps for each decision-maker and data practitioner are shared in this section.

```{r, echo=FALSE, fig.height=4.1, fig.with=3}

# Define the paths to maps
image4 <- "D:/Morgan/Documents/NCL MSc/final_project/mofrod_project/graphs/userresearch/Decision-maker 1_ Concept Map.png"

# Read the images
img4 <- image_read(image4)

# Convert images to raster
img4_raster <- as.raster(img4)

# Convert raster to grobs
img4_grob <- rasterGrob(img4_raster)

# Create a text grob for the title with custom font settings
title_grob4 <- textGrob(
  "Figure 8: ",
  x = 0.051, 
  y= 0.5,
  gp = gpar(fontfamily = "serif", fontsize = 11)
)

# Arrange the images side by side
grid.arrange(img4_grob, top=title_grob4)

```

```{r, echo=FALSE, fig.height=4.1, fig.with=3}

# Define the paths to maps
image5 <- "D:/Morgan/Documents/NCL MSc/final_project/mofrod_project/graphs/userresearch/Decision-maker 2_ Concept Map.png"

# Read the images
img5 <- image_read(image5)

# Convert images to raster
img5_raster <- as.raster(img5)

# Convert raster to grobs
img5_grob <- rasterGrob(img5_raster)

# Create a text grob for the title with custom font settings
title_grob5 <- textGrob(
  "Figure 9: ",
  x = 0.051, 
  y= 0.5,
  gp = gpar(fontfamily = "serif", fontsize = 11)
)

# Arrange the images side by side
grid.arrange(img5_grob, top=title_grob5)

```

```{r, echo=FALSE, fig.height=4.1, fig.with=3}

# Define the paths to maps
image6 <- "D:/Morgan/Documents/NCL MSc/final_project/mofrod_project/graphs/userresearch/Decision-maker 3_ Concept Map.png"

# Read the images
img6 <- image_read(image6)

# Convert images to raster
img6_raster <- as.raster(img6)

# Convert raster to grobs
img6_grob <- rasterGrob(img6_raster)

# Create a text grob for the title with custom font settings
title_grob6 <- textGrob(
  "Figure 10: ",
  x = 0.051, 
  y= 0.5,
  gp = gpar(fontfamily = "serif", fontsize = 11)
)

# Arrange the images side by side
grid.arrange(img6_grob, top=title_grob6)

```

```{r, echo=FALSE, fig.height=4.1, fig.with=3}

# Define the paths to maps
image7 <- "D:/Morgan/Documents/NCL MSc/final_project/mofrod_project/graphs/userresearch/Decision-maker 4_ Concept Map.png"

# Read the images
img7 <- image_read(image7)

# Convert images to raster
img7_raster <- as.raster(img7)

# Convert raster to grobs
img7_grob <- rasterGrob(img7_raster)

# Create a text grob for the title with custom font settings
title_grob7 <- textGrob(
  "Figure 11: ",
  x = 0.051, 
  y= 0.5,
  gp = gpar(fontfamily = "serif", fontsize = 11)
)

# Arrange the images side by side
grid.arrange(img7_grob, top=title_grob7)

```

```{r, echo=FALSE, fig.height=4.1, fig.with=3}

# Define the paths to maps
image8 <- "D:/Morgan/Documents/NCL MSc/final_project/mofrod_project/graphs/userresearch/Decision-maker 5_ Concept Map.png"

# Read the images
img8 <- image_read(image8)

# Convert images to raster
img8_raster <- as.raster(img8)

# Convert raster to grobs
img8_grob <- rasterGrob(img8_raster)

# Create a text grob for the title with custom font settings
title_grob8 <- textGrob(
  "Figure 12:",
  x = 0.051, 
  y= 0.5,
  gp = gpar(fontfamily = "serif", fontsize = 11)
)

# Arrange the images side by side
grid.arrange(img8_grob, top=title_grob8)

```

```{r, echo=FALSE, fig.height=4.1, fig.with=3}

# Define the paths to maps
image9 <- "D:/Morgan/Documents/NCL MSc/final_project/mofrod_project/graphs/userresearch/Decision-maker 6_ Concept Map.png"

# Read the images
img9 <- image_read(image9)

# Convert images to raster
img9_raster <- as.raster(img9)

# Convert raster to grobs
img9_grob <- rasterGrob(img9_raster)

# Create a text grob for the title with custom font settings
title_grob9 <- textGrob(
  "Figure 13: ",
  x = 0.051, 
  y= 0.5,
  gp = gpar(fontfamily = "serif", fontsize = 11)
)

# Arrange the images side by side
grid.arrange(img9_grob, top=title_grob9)

```

```{r, echo=FALSE, fig.height=4.1, fig.with=3}

# Define the paths to maps
image10 <- "D:/Morgan/Documents/NCL MSc/final_project/mofrod_project/graphs/userresearch/Decision-maker 7_ Concept Map.png"

# Read the images
img10 <- image_read(image10)

# Convert images to raster
img10_raster <- as.raster(img10)

# Convert raster to grobs
img10_grob <- rasterGrob(img10_raster)

# Create a text grob for the title with custom font settings
title_grob10 <- textGrob(
  "Figure 14: ",
  x = 0.051, 
  y= 0.5,
  gp = gpar(fontfamily = "serif", fontsize = 11)
)

# Arrange the images side by side
grid.arrange(img10_grob, top=title_grob10)

```

```{r, echo=FALSE, fig.height=4.1, fig.with=3}

# Define the paths to maps
image11 <- "D:/Morgan/Documents/NCL MSc/final_project/mofrod_project/graphs/userresearch/Decision-maker 8_ Concept Map.png"

# Read the images
img11 <- image_read(image11)

# Convert images to raster
img11_raster <- as.raster(img11)

# Convert raster to grobs
img11_grob <- rasterGrob(img11_raster)

# Create a text grob for the title with custom font settings
title_grob11 <- textGrob(
  "Figure 15: ",
  x = 0.051, 
  y= 0.5,
  gp = gpar(fontfamily = "serif", fontsize = 11)
)

# Arrange the images side by side
grid.arrange(img11_grob, top=title_grob11)

```

```{r, echo=FALSE, fig.height=4.1, fig.with=3}

# Define the paths to maps
image12 <- "D:/Morgan/Documents/NCL MSc/final_project/mofrod_project/graphs/userresearch/Decision-maker 9_ Concept Map.png"

# Read the images
img12 <- image_read(image12)

# Convert images to raster
img12_raster <- as.raster(img12)

# Convert raster to grobs
img12_grob <- rasterGrob(img12_raster)

# Create a text grob for the title with custom font settings
title_grob12 <- textGrob(
  "Figure 16: ",
  x = 0.051, 
  y= 0.5,
  gp = gpar(fontfamily = "serif", fontsize = 11)
)

# Arrange the images side by side
grid.arrange(img12_grob, top=title_grob12)

```

```{r, echo=FALSE, fig.height=4.1, fig.with=3}

# Define the paths to maps
image13 <- "D:/Morgan/Documents/NCL MSc/final_project/mofrod_project/graphs/userresearch/Data Practitioner 1_ Concept Map.png"

# Read the images
img13 <- image_read(image13)

# Convert images to raster
img13_raster <- as.raster(img13)

# Convert raster to grobs
img13_grob <- rasterGrob(img13_raster)

# Create a text grob for the title with custom font settings
title_grob13 <- textGrob(
  "Figure 17: ",
  x = 0.051, 
  y= 0.5,
  gp = gpar(fontfamily = "serif", fontsize = 11)
)

# Arrange the images side by side
grid.arrange(img13_grob, top=title_grob13)

```

```{r, echo=FALSE, fig.height=4.1, fig.with=3}

# Define the paths to maps
image14 <- "D:/Morgan/Documents/NCL MSc/final_project/mofrod_project/graphs/userresearch/Data Practitioner 2_ Concept Map.png"

# Read the images
img14 <- image_read(image14)

# Convert images to raster
img14_raster <- as.raster(img14)

# Convert raster to grobs
img14_grob <- rasterGrob(img14_raster)

# Create a text grob for the title with custom font settings
title_grob14 <- textGrob(
  "Figure 18: ",
  x = 0.051, 
  y= 0.5,
  gp = gpar(fontfamily = "serif", fontsize = 11)
)

# Arrange the images side by side
grid.arrange(img14_grob, top=title_grob14)

```

```{r, echo=FALSE,  fig.height=4.1, fig.with=3}

# Define the paths to maps
image15 <- "D:/Morgan/Documents/NCL MSc/final_project/mofrod_project/graphs/userresearch/Data Practitioner 3_ Concept Map.png"

# Read the images
img15 <- image_read(image15)

# Convert images to raster
img15_raster <- as.raster(img15)

# Convert raster to grobs
img15_grob <- rasterGrob(img15_raster)

# Create a text grob for the title with custom font settings
title_grob15 <- textGrob(
  "Figure 19: ",
  x = 0.051, 
  y= 0.5,
  gp = gpar(fontfamily = "serif", fontsize = 11)
)

# Arrange the images side by side
grid.arrange(img15_grob, top=title_grob15)

```

```{r, echo=FALSE, fig.height=4.1, fig.with=3}

# Define the paths to maps
image16 <- "D:/Morgan/Documents/NCL MSc/final_project/mofrod_project/graphs/userresearch/Data Practitioner 4_ Concept Map.png"

# Read the images
img16 <- image_read(image16)

# Convert images to raster
img16_raster <- as.raster(img16)

# Convert raster to grobs
img16_grob <- rasterGrob(img16_raster)

# Create a text grob for the title with custom font settings
title_grob16 <- textGrob(
  "Figure 20: ",
  x = 0.051, 
  y= 0.5,
  gp = gpar(fontfamily = "serif", fontsize = 11)
)

# Arrange the images side by side
grid.arrange(img16_grob, top=title_grob16)

```

```{r, echo=FALSE, fig.height=4.1, fig.with=3}

# Define the paths to maps
image17 <- "D:/Morgan/Documents/NCL MSc/final_project/mofrod_project/graphs/userresearch/Data Practitioner 5_ Concept Map.png"

# Read the images
img17 <- image_read(image17)

# Convert images to raster
img17_raster <- as.raster(img17)

# Convert raster to grobs
img17_grob <- rasterGrob(img17_raster)

# Create a text grob for the title with custom font settings
title_grob17 <- textGrob(
  "Figure 21: ",
  x = 0.051, 
  y= 0.5,
  gp = gpar(fontfamily = "serif", fontsize = 11)
)

# Arrange the images side by side
grid.arrange(img17_grob, top=title_grob17)

```

```{r, echo=FALSE, fig.height=4.1, fig.with=3}

# Define the paths to maps
image18 <- "D:/Morgan/Documents/NCL MSc/final_project/mofrod_project/graphs/userresearch/Data Practitioner 6_ Concept Map.png"

# Read the images
img18 <- image_read(image18)

# Convert images to raster
img18_raster <- as.raster(img18)

# Convert raster to grobs
img18_grob <- rasterGrob(img18_raster)

# Create a text grob for the title with custom font settings
title_grob18 <- textGrob(
  "Figure 22: ",
  x = 0.051, 
  y= 0.5,
  gp = gpar(fontfamily = "serif", fontsize = 11)
)

# Arrange the images side by side
grid.arrange(img18_grob, top=title_grob18)

```

```{r, echo=FALSE, fig.height=4.1, fig.with=3}

# Define the paths to maps
image19 <- "D:/Morgan/Documents/NCL MSc/final_project/mofrod_project/graphs/userresearch/Data Practitioner 7_ Concept Map.png"

# Read the images
img19 <- image_read(image19)

# Convert images to raster
img19_raster <- as.raster(img19)

# Convert raster to grobs
img19_grob <- rasterGrob(img19_raster)

# Create a text grob for the title with custom font settings
title_grob19 <- textGrob(
  "Figure 23: ",
  x = 0.051, 
  y= 0.5,
  gp = gpar(fontfamily = "serif", fontsize = 11)
)

# Arrange the images side by side
grid.arrange(img19_grob, top=title_grob19)

```

\newpage
# Annex D: Original Nine Synthetic Time Series

\vspace{-0.4cm}

Each line plot in Figure 24 below visualises one of the nine original synthetic time series from the Alan Turing Institute @ATIChangePoint.

```{r echo=FALSE, fig.height=8, fig.with=3, message=FALSE, warning=FALSE, paged.print=FALSE}
ggplot(merged, 
       aes(x = id, y = val)) + geom_line(linewidth = 1, color = "#084594") + facet_grid(dataset~., scales = "free") +
    theme_minimal() +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -0.26, size=11)) +
  labs(x = "Time Period", y = "Time Value", title = "Figure 24: Annotated Change Nine Synthetic Time Series")
```

\newpage
# Annex E: Coefficient of Variation for all `Catch22` Features

\vspace{-0.3cm}

Figure 25 visualises the absolute coefficient of variation values for each `catch22` time series feature.

```{r, echo=FALSE, fig.height=8, fig.with=3}
# Reshape sensitivity data to a long format
cv_sensitivity_joined_long <- cv_sensitivity_joined %>%
  pivot_longer(cols = c(everyNthSensitivity, PercentageChangeSensitivity), 
               names_to = "Method", 
               values_to = "CoefficientofVariation")

# Change the Method into a factor and set the levels
cv_sensitivity_joined_long$Method <- factor(cv_sensitivity_joined_long$Method, levels = c("everyNthSensitivity", "PercentageChangeSensitivity"))

# Recode the names of the Rcatch22 features
cv_sensitivity_joined_long$names <- recode(cv_sensitivity_joined_long$names, 
                                           'DN_HistogramMode_5' = "HistogramBin5",
                                           'DN_HistogramMode_10' = "HistogramBin10",
                                           'CO_f1ecac' = "Autocorr_ApproxScale",
                                           'CO_FirstMin_ac' = "Autocorr_FirstMinimum",
                                           'CO_HistogramAMI_even_2_5' = "NonLinear_HistogramBin5",
                                           'CO_trev_1_num' = "CubeDifference_Average",
                                           'MD_hrv_classic_pnn40' = "DifferenceProportions_Above4%",
                                           'SB_BinaryStats_mean_longstretch1' = "LongestPeriod_AboveAverage",
                                           'SB_TransitionMatrix_3ac_sumdiagcov' = "Transition_Column-wiseVariances",
                                           'PD_PeriodicityWang_th0_01' = "Autocorr_FirstPeak",
                                           'CO_Embed2_Dist_tau_d_expfit_meandiff' = "DistributionExponentialFit_MAE",
                                           'IN_AutoMutualInfoStats_40_gaussian_fmmi' = "Autocorr_Automutual",
                                           'FC_LocalSimple_mean1_tauresrat' = "Ratio_Differences_Zero-crossing",
                                           'DN_OutlierInclude_p_001_mdrmd' = "Over-threshold_Positive",
                                           'DN_OutlierInclude_n_001_mdrmd' = "Over-threshold_Negative",
                                           'SP_Summaries_welch_rect_area_5_1' = "PowerSpectrum_Lowest20%",
                                           'SB_BinaryStats_diff_longstretch0' = "LongestPeriod_SuccessiveDecreases",
                                           'SB_MotifThree_quantile_hh' = "ThreeSymbolProbability_Entropy",
                                           'SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1' = "LogarithmicTimescaleFluctuation",
                                           'SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1' = "DetrendedTimescaleFluctuation",
                                           'SP_Summaries_welch_rect_centroid' = "PowerSpectrum_Centroid",
                                           'FC_LocalSimple_mean3_stderr' = "ErrorMeasure_Previous3Values", 
                                           'DN_Spread_Std' = 'Spead',
                                           'DN_Mean' = 'Mean')

# Recode the names of the Methods
cv_sensitivity_joined_long$Method <- recode(cv_sensitivity_joined_long$Method,  
                                            "PercentageChangeSensitivity" = "Percentage Change", 
                                            "everyNthSensitivity" = "EveryNth")

# Arrange order
cv_sensitivity_joined_long <- cv_sensitivity_joined_long %>%
  arrange(CoefficientofVariation)

# Convert "names" to a factor
cv_sensitivity_joined_long$names <- factor(cv_sensitivity_joined_long$names, levels = unique(cv_sensitivity_joined_long$names))

# Create a bar plot
ggplot(cv_sensitivity_joined_long, aes(x = CoefficientofVariation, y = names, fill = Method)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  theme_minimal() +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -2.6, size=11), legend.position = "top", legend.justification = c(-1.05, 1)) +
  labs(x = "Coefficient of Variation", y = "Feature", fill = "Method", title = "Figure 25: Coefficient of Variation for Catch22 Features") +
  scale_fill_brewer(palette = "Paired")

```

# Annex F: The Statistical Properties of `Catch22` Features
\renewcommand{\arraystretch}{1.5}
```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.height=8, fig.width=3}

# Manually create a table 
table_content <- matrix(
  c(
    "Distribution shape", "DN_HistogramMode_5", "HistogramBin5", "5-bin histogram mode",
    "Distribution shape", "DN_HistogramMode_10", "HistogramBin10", "10-bin histogram mode",
    "Extreme event timing", "DN_OutlierInclude_p_001_ mdrmd", "Over-threshold_ Positive", "Positive outlier timing",
    "Extreme event timing", "DN_OutlierInclude_n_001_ mdrmd", "Over-threshold_ Negative", "Negative outlier timing",
    "Linear autocorrelation", "CO_f1ecac", "Autocorr_ApproxScale", "First  crossing of the ACF",
    "Linear autocorrelation", "CO_FirstMin_ac", "Autocorr_FirstMinimum", "First minimum of the ACF",
    "Linear autocorrelation structure", "SP_Summaries_welch_rect_ area_5_1", "PowerSpectrum_Lowest20%", "Power in lowest 20% frequencies",
    "Linear autocorrelation", "SP_Summaries_welch_rect_ centroid", "PowerSpectrum_Centroid", "Centroid frequency",
    "Linear autocorrelation", "PD_PeriodicityWang_th001", "Autocorr_FirstPeak", "Wang's periodicity metric",
    "Simple forcasting", "FC_LocalSimple_mean3_ stderr", "ErrorMeasure_Previous3Values", "Error of 3-point rolling mean forecast",
    "Incremental differences", "FC_LocalSimple_mean1_ tauresrat", "Ratio_Differences_Zero-crossing", "Change in autocorrelation timescale after incremental differencing",
    "Incremental differences", "MD_hrv_classic_ pnn40", "DifferenceProportions_Above4%", "Proportion of high incremental changes in the series",
    "Symbolic", "SB_BinaryStats_mean_ longstretch1", "LongestPeriod_AboveAverage", "Longest stretch of above-mean values",
    "Symbolic", "SB_BinaryStats_diff_ longstretch0", "LongestPeriod_SuccessiveDecreases", "Longest stretch of decreasing values"
  ), nrow = 14, byrow = TRUE
)

# Define the column names
colnames(table_content) <- c("Category", "Original Feature Name", "Other Feature Name", "Description")

# Create the table with the title
knitr::kable(table_content, caption = "Catch22 Feature Overview [47]") %>%
  kable_classic_2(full_width = F) %>%
    column_spec(1, width = "0.7in") %>%
  column_spec(2, width = "2.1in") %>%
  column_spec(3, width = "2.1in") %>%
  column_spec(4, width = "1in") %>%
  kable_styling("striped", latex_options = "HOLD_position")
```

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.height=8, fig.width=3}

# Manually create a table 
table_content_cont <- matrix(
  c(
    "Symbolic", "SB_MotifThree_quantile_hh", "ThreeSymbolProbability_Entropy", "Entropy of successive pairs in symbolized series",
    "Symbolic", "SB_TransitionMatrix_3ac_sumdiagcov", "Transition_Column-wiseVariances", "Transition matrix column variance",
    "Non-linear autocorrelation", "CO_HistogramAMI_even_2_5", "NonLinear_HistogramBin5", "Histogram-based automutual information (lag 2, 5 bins)",
    "Non-linear autocorrelation", "CO_trev_1_num", "CubeDifference_Average", "Time reversibility",
    "Non-linear autocorrelation", "IN_AutoMutualInfoStats_40_ gaussian_fmmi", "Autocorr_Automutual", "First minimum of the AMI function",
    "Self-affine scaling", "SC_FluctAnal_2_rsrangefit_50_1_ logi_prop_r1", "LogarithmicTimescaleFluctuation", "Rescaled range fluctuation analysis (low-scale scaling)",
    "Self-affine scaling", "SC_FluctAnal_2_dfa_50_1_2_logi_ prop_r1", "DetrendedTimescaleFluctuation", "Detrended fluctuation analysis (low-scale scaling)",
    "Other", "CO_Embed2_Dist_tau_d_expfit_ meandiff", "DistributionExponentialFit_MAE", "Goodness of exponential fit to embedding distance distribution",
    "Other", "DN_Mean", "Mean", "Mean",
    "Other", "DN_Spread_Std", "Spread", "Standard Deviation"
  ), nrow = 10, byrow = TRUE
)

# Define the column names
colnames(table_content) <- c("Category", "Original Feature Name", "Other Feature Name", "Description")

# Create the table with the title
knitr::kable(table_content_cont, caption = "Catch22 Feature Overview Continued [47]") %>%
  kable_classic_2(full_width = F) %>%
    column_spec(1, width = "0.7in") %>%
  column_spec(2, width = "2.3in") %>%
  column_spec(3, width = "2.1in") %>%
  column_spec(4, width = "1in") %>%
  kable_styling("striped", latex_options = "HOLD_position")
```

It is interesting to note that the seven most sensitive `catch22` features are in three categories: `symbolic`, `linear autocorrelation`, and `other`. Further investigation to understand why would be beneficial. 

\newpage

# Annex G: Variation of `Catch22` Features across Time Series Types

Figure 26 presents feature with the highest coefficient of variation across the 900 time series is the longest sequence of successive steps that decrease (`LongestPeriod_Decreases` or `SB_BinaryStats_diff_longstretch0`). This catch22 feature *"calculates the longest sequence of successive steps in the time series that decrease"* @feature_book. Initial visual analysis suggests that time series '100', '300', and '500' are most affected by this feature.

```{r LongestDecrease, echo=FALSE, message=FALSE, warning=FALSE, fig.height=6, fig.with=3}
# Create the scaled difference line graph for SB_BinaryStats_diff_longstretch0
ggplot(filtered_df %>%
         filter(names == "LongestPeriod_Decreases"), 
       aes(x = as.numeric(vol), y = scaled, color = method)) +
  scale_color_brewer(palette = "Paired") +
  geom_line(linewidth = 0.5) +
  scale_x_reverse() +
  theme_minimal() +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -0.61, size=11), legend.position = "top", legend.justification = c(-0.04, 1)) +
  labs(x = "Volume of Data", y = "Scaled Difference", color = "Method", title = "Figure 26: Variation of the Longest Sequence of Successive Steps that Decrease") +
  facet_wrap(~ dataset, ncol = 3)
```

\newpage

Figure 27 presents feature with the second highest coefficient of variation is the longest sequence of successive steps that decrease (`Autocorr_FirstMinimum` or `CO_FirstMin_ac`). This catch22 feature represents *"calculates the longest sequence of successive steps in the time series that decrease”* @feature_book. Initial visual analysis suggests that time series '100', '200' and '500' are more affected by this feature.

```{r FirstMinimum2, echo=FALSE, message=FALSE, warning=FALSE, fig.height=6, fig.with=3}

# Create the difference line graph for CO_FirstMin_ac
ggplot(filtered_df %>%
         filter(names == "Autocorr_FirstMinimum"), 
       aes(x = as.numeric(vol), y = scaled, color = method)) +
  scale_color_brewer(palette = "Paired") +
  geom_line(linewidth = 0.5) +
  scale_x_reverse() +
  theme_minimal() +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -0.32, size=11), legend.position = "top", legend.justification = c(-0.04, 1)) +
  labs(x = "Volume of Data", y = "Scaled Difference", color = "Method", title = "Figure 27: Variation of the First Minimum of the Autocorrelation Function") +
  facet_wrap(~ dataset, ncol = 3)

```

\newpage

Figure 28 presents the feature with the third highest coefficient of variation is the minimum of the automutual information function (`Autocorr_Automutual` or `IN_AutoMutualInfoStats_40_gaussian_fmmi`). This catch22 feature outputs a measure of *"autocorrelation in the time series, as the minimum of the automutual information function"* @feature_book. Initial visual analysis suggests that time series '100', '200', '500', and '600' are more affected by this feature.

```{r AutoMutalFunction, echo=FALSE, message=FALSE, warning=FALSE, fig.height=6, fig.with=3}
# Create the difference line graph for IN_AutoMutualInfoStats_40_gaussian_fmmi
ggplot(filtered_df %>%
         filter(names == "Autocorr_Automutual"), 
       aes(x = as.numeric(vol), y = difference, color = method)) +
  scale_color_brewer(palette = "Paired") +
  geom_line(linewidth = 0.5) +
  scale_x_reverse() +
  theme_minimal() +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -0.45, size=11), legend.position = "top", legend.justification = c(-0.04, 1)) +
  labs(x = "Volume of Data", y = "Scaled Difference", color = "Method", title = "Figure 28: Variation of the Minimum of the Automutual Information Function") +
  facet_wrap(~ dataset, ncol = 3)

```

\newpage

Figure 29 presents the feature with the fourth highest coefficient of variation is the longest sequence of successive values greater than the mean (`LongestPeriod_AboveAverage` or `SB_BinaryStats_mean_longstretch1`). This catch22 feature *"calculates the longest successive period of above average values"* @feature_book. Initial visual analysis suggests that time series '200', '500', and '800B' are more affected by this feature.

```{r LongestGreater, echo=FALSE, message=FALSE, warning=FALSE, fig.height=6, fig.with=3}
# Create the line graph for SB_BinaryStats_mean_longstretch1
ggplot(filtered_df %>%
         filter(names == "LongestPeriod_AboveAverage"), 
       aes(x = as.numeric(vol), y = scaled, color = method)) +
  scale_color_brewer(palette = "Paired") +
  geom_line(linewidth = 0.5) +
  scale_x_reverse() +
  theme_minimal() +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -1.4, size=11), legend.position = "top", legend.justification = c(-0.04, 1)) +
  labs(x = "Volume of Data", y = "Scaled Difference", color = "Method", title = "Figure 29: Variation of the Longest Sequence of Successive Values Greater than the Mean") +
  facet_wrap(~ dataset, ncol = 3)
```

\newpage

Figure 30 presents the feature with the fifth highest coefficient of variation is the approximate scale of autocorrelation (`Autocorr_ApproxScale` or `CO_f1ecac`). This catch22 feature *"computes the first 1/e crossing of the autocorrelation function of the time series"* @feature_book. It is similar to the first minimum of the autocorrelation function. Initial visual analysis suggests that time series '100', '200', '500', '800A' and '800B' are more affected by this feature.

```{r ApproxScale, echo=FALSE, message=FALSE, warning=FALSE, fig.height=6, fig.with=3}
# Create the difference line graph for CO_f1ecac
ggplot(filtered_df %>%
         filter(names == "Autocorr_ApproxScale"), 
       aes(x = as.numeric(vol), y = difference, color = method)) +
  scale_color_brewer(palette = "Paired") +
  geom_line(linewidth = 0.5) +
  scale_x_reverse() +
  theme_minimal() +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -0.27, size=11), legend.position = "top", legend.justification = c(-0.04, 1)) +
  labs(x = "Volume of Data", y = "Scaled Difference", color = "Method", title = "Figure 30: Variation of the Approximate Scale of Autocorrelation") +
  facet_wrap(~ dataset, ncol = 3)
```

\newpage

Figure 31 presents the feature with the sixth highest coefficient of variation is the first peak in the autocorrelation function (`Autocorr_FirstPeak` or `PD_PeriodicityWang_th0_01`). This catch22 feature *"returns the first peak in the autocorrelation function satisfying a set of conditions (after detrending the time series using a single-knot cubic regression spline)"* @feature_book. In general, the feature returns higher values for slower time series oscillation. Initial visual analysis suggests that time series '100', '200' and '500' are more affected by this feature.

```{r FirstPeak, echo=FALSE, message=FALSE, warning=FALSE, fig.height=6, fig.with=3}
# Create the difference line graph for PD_PeriodicityWang_th0_01 
ggplot(filtered_df %>%
         filter(names == "Autocorr_FirstPeak"), 
       aes(x = as.numeric(vol), y = difference, color = method)) +
  scale_color_brewer(palette = "Paired") +
  geom_line(linewidth = 0.5) +
  scale_x_reverse() +
  theme_minimal() +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -0.3, size=11), legend.position = "top", legend.justification = c(-0.04, 1)) +
  labs(x = "Volume of Data", y = "Scaled Difference", color = "Method", title = "Figure 31: Variation of the First Peak in the Autocorrelation Function") +
  facet_wrap(~ dataset, ncol = 3)
```

\newpage

Figure 32 presents the feature with the seventh highest coefficient of variation is the mean absolute error of an exponential fit to a probability distribution (`DistributionExponentialFit_MAE` or `CO_Embed2_Dist_tau_d_expfit_meandiff`). To calculate the mean absolute error, this catch22 feature *"represents the time series in a two-dimensional time-delay embedding space... It then computes successive distances between points in this 2D embedding space and analyzes the probability distribution of these distances"* @feature_book. Initial visual analysis suggests that time series '600' and '800' are more affected by this feature.

```{r MAE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=6, fig.with=3}
# Create the difference line graph for CO_Embed2_Dist_tau_d_expfit_meandiff
ggplot(filtered_df %>%
         filter(names == "DistributionExponentialFit_MAE"), 
       aes(x = as.numeric(vol), y = difference, color = method)) +
  scale_color_brewer(palette = "Paired") +
  geom_line(linewidth = 0.5) +
  scale_x_reverse() +
  theme_minimal() +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -0.3, size=11), legend.position = "top", legend.justification = c(-0.04, 1)) +
  labs(x = "Volume of Data", y = "Scaled Difference", color = "Method", title = "Figure 32: Variation of the Mean Absolute Error of an Exponential Fit") +
  facet_wrap(~ dataset, ncol = 3)
```

\newpage
# Annex H: Most Sensitive `Catch22` Features for each Time Series Type

Figure 33 visualises how the six most sensitive features for time series 100. Initial visual analysis suggests that `Autocorr_FirstMinimum`, `Autocorr_FirstPeak` and `LongestPeriod_Decreases` may best indicate the impact of downsampling for this time series.

```{r Catch22Variation100, echo=FALSE, message=FALSE, warning=FALSE, fig.height=6, fig.with=3}

# Create the difference line graph for 100
ggplot(subset_filtered_df %>%
         filter(dataset == "100"), 
       aes(x = as.numeric(vol), y = scaled, color = method)) +
  scale_color_brewer(palette = "Paired") +
  geom_line(linewidth = 0.5) +
  scale_x_reverse() +
  theme_minimal() +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -0.2, size=11), legend.position = "top", legend.justification = c(-0.04, 1)) +
  labs(x = "Volume of Data", y = "Scaled Difference", color = "Method", title = "Figure 33: Variation of Catch22 Features for Time Series 100") +
  facet_wrap(~ names, ncol = 3)
```

\newpage

Figure 34 visualises how the six most sensitive features for time series 200. `Autocorr_Automutual`, `Autocorr_FirstMinimum`, `Autocorr_FirstPeak` and `LongestPeriod_AboveAverage` may best indicate the impact of downsampling for this time series.

```{r Catch22Variation200, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3.9, fig.with=3}

# Create the difference line graph for 200
ggplot(subset_filtered_df %>%
         filter(dataset == "200"), 
       aes(x = as.numeric(vol), y = scaled, color = method)) +
  scale_color_brewer(palette = "Paired") +
  geom_line(linewidth = 0.5) +
  scale_x_reverse() +
  theme_minimal() +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -0.2, size=11), legend.position = "top", legend.justification = c(-0.04, 1)) +
  labs(x = "Volume of Data", y = "Scaled Difference", color = "Method", title = "Figure 34: Variation of Catch22 Features for Time Series 200") +
  facet_wrap(~ names, ncol = 3)
```

Figure 35 visualises how the six most sensitive features for time series 300. `Autocorr_FirstPeak`, `LongestPeriod_AboveAverage` and `LongestPeriod_Decreases` may best indicate the impact of downsampling for this time series.

```{r Catch22Variation300, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3.9, fig.with=3}

# Create the difference line graph for 300
ggplot(subset_filtered_df %>%
         filter(dataset == "300"), 
       aes(x = as.numeric(vol), y = scaled, color = method)) +
  scale_color_brewer(palette = "Paired") +
  geom_line(linewidth = 0.5) +
  scale_x_reverse() +
  theme_minimal() +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -0.2, size=11), legend.position = "top", legend.justification = c(-0.04, 1)) +
  labs(x = "Volume of Data", y = "Scaled Difference", color = "Method", title = "Figure 35: Variation of Catch22 Features for Time Series 300") +
  facet_wrap(~ names, ncol = 3)
```

Figure 36 visualises how the six most sensitive features for time series 400. `Autocorr_FirstPeak` and `LongestPeriod_Decreases` may best indicate the impact of downsampling for this time series.

```{r Catch22Variation400, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3.9, fig.with=3}

# Create the difference line graph for 400
ggplot(subset_filtered_df %>%
         filter(dataset == "400"), 
       aes(x = as.numeric(vol), y = scaled, color = method)) +
  scale_color_brewer(palette = "Paired") +
  geom_line(linewidth = 0.5) +
  scale_x_reverse() +
  theme_minimal() +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -0.2, size=11), legend.position = "top", legend.justification = c(-0.04, 1)) +
  labs(x = "Volume of Data", y = "Scaled Difference", color = "Method", title = "Figure 36: Variation of Catch22 Features for Time Series 400") +
  facet_wrap(~ names, ncol = 3)
```

Figure 37 visualises how the six most sensitive features for time series 500. `Autocorr_Automutual`, `Autocorr_FirstMinimum`, and `Autocorr_FirstPeak` may best indicate the impact of downsampling for this time series.

```{r Catch22Variation500, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3.9, fig.with=3}

# Create the difference line graph for 500
ggplot(subset_filtered_df %>%
         filter(dataset == "500"), 
       aes(x = as.numeric(vol), y = scaled, color = method)) +
  scale_color_brewer(palette = "Paired") +
  geom_line(linewidth = 0.5) +
  scale_x_reverse() +
  theme_minimal() +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -0.2, size=11), legend.position = "top", legend.justification = c(-0.04, 1)) +
  labs(x = "Volume of Data", y = "Scaled Difference", color = "Method", title = "Figure 37: Variation of Catch22 Features for Time Series 500") +
  facet_wrap(~ names, ncol = 3)
```

\newpage

Figure 38 visualises how the six most sensitive features for time series 600. `Autocorr_Automutual` and `Autocorr_FirstMinimum` may best indicate the impact of downsampling for this time series.

```{r Catch22Variation600, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3.9, fig.with=3}

# Create the difference line graph for 600
ggplot(subset_filtered_df %>%
         filter(dataset == "600"), 
       aes(x = as.numeric(vol), y = scaled, color = method)) +
  scale_color_brewer(palette = "Paired") +
  geom_line(linewidth = 0.5) +
  scale_x_reverse() +
  theme_minimal() +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -0.2, size=11), legend.position = "top", legend.justification = c(-0.04, 1)) +
  labs(x = "Volume of Data", y = "Scaled Difference", color = "Method", title = "Figure 38: Variation of Catch22 Features for Time Series 600") +
  facet_wrap(~ names, ncol = 3)
```

Figure 39 visualises how the six most sensitive features for time series 700. `LongestPeriod_AboveAverage` and `LongestPeriod_Decreases` may best indicate the impact of downsampling for this time series.

```{r Catch22Variation700, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3.9, fig.with=3}

# Create the difference line graph for 700
ggplot(subset_filtered_df %>%
         filter(dataset == "700"), 
       aes(x = as.numeric(vol), y = scaled, color = method)) +
  scale_color_brewer(palette = "Paired") +
  geom_line(linewidth = 0.5) +
  scale_x_reverse() +
  theme_minimal() +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -0.2, size=11), legend.position = "top", legend.justification = c(-0.04, 1)) +
  labs(x = "Volume of Data", y = "Scaled Difference", color = "Method", title = "Figure 39: Variation of Catch22 Features for Time Series 700") +
  facet_wrap(~ names, ncol = 3)
```

\newpage

Figure 40 visualises how the six most sensitive features for time series 800A. `Autocorr_ApproxScale` and `LongestPeriod_Decreases` may best indicate the impact of downsampling for this time series.

```{r Catch22Variation800A, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3.9, fig.with=3}

# Create the difference line graph for 800A
ggplot(subset_filtered_df %>%
         filter(dataset == "800A"), 
       aes(x = as.numeric(vol), y = scaled, color = method)) +
  scale_color_brewer(palette = "Paired") +
  geom_line(linewidth = 0.5) +
  scale_x_reverse() +
  theme_minimal() +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -0.2, size=11), legend.position = "top", legend.justification = c(-0.04, 1)) +
  labs(x = "Volume of Data", y = "Scaled Difference", color = "Method", title = "Figure 40: Variation of Catch22 Features for Time Series 800A") +
  facet_wrap(~ names, ncol = 3)
```

Figure 41 visualises how the six most sensitive features for time series 800A. `Autocorr_ApproxScale`, `Autocorr_FirstMinimum`, `LongestPeriod_AboveAverage` and `LongestPeriod_Decreses` may best indicate the impact of downsampling for this time series.

```{r Catch22Variation800B, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3.9, fig.with=3}

# Create the difference line graph for 800B
ggplot(subset_filtered_df %>%
         filter(dataset == "800B"), 
       aes(x = as.numeric(vol), y = scaled, color = method)) +
  scale_color_brewer(palette = "Paired") +
  geom_line(linewidth = 0.5) +
  scale_x_reverse() +
  theme_minimal() +
  theme(text = element_text(family = "Times"), plot.title = element_text(hjust = -0.2, size=11), legend.position = "top", legend.justification = c(-0.04, 1)) +
  labs(x = "Volume of Data", y = "Scaled Difference", color = "Method", title = "Figure 41: Variation of Catch22 Features for Time Series 800B") +
  facet_wrap(~ names, ncol = 3)
```

\newpage
# REFERENCES





